{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import top_anomalous, top_mean_anomalous, errorbar_classification\n",
    "from util import _calc_grid_size, tile_tv_images\n",
    "\n",
    "from verification_net import VerificationNet\n",
    "from model_trainer import ModelTrainer\n",
    "from autoencoder import AutoEncoder\n",
    "\n",
    "from stacked_mnist import StackedMNIST, DataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "torch.mps.set_per_process_memory_fraction(0.)\n",
    "\n",
    "trainer_file = Path(\"trainers/ae-basic.pkl\")\n",
    "model_file = Path(\"models/ae-basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAE(AutoEncoder):\n",
    "    def __init__(self, latent_space_size: int = 64) -> None:\n",
    "        super().__init__(latent_space_size)\n",
    "        self.latent_space_size = latent_space_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        nb_channels = x.shape[1]\n",
    "        x_hats = []\n",
    "        z_hats = []\n",
    "        for channel in range(nb_channels):\n",
    "            z_hat = self.encoder(x[:, [channel], :, :]).view(-1,1,self.latent_space_size)\n",
    "            x_hat = self.decoder(z_hat.view(-1,self.latent_space_size))\n",
    "            z_hats.append(z_hat)\n",
    "            x_hats.append(x_hat)\n",
    "\n",
    "        z_hats = torch.cat(z_hats, dim=1)\n",
    "        x_hats = torch.cat(x_hats, dim=1)\n",
    "        return z_hats, x_hats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer(ModelTrainer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            loss, \n",
    "            optimizer,\n",
    "            device = torch.device(\"mps\"),\n",
    "            file_name: str | Path = model_file, \n",
    "            force_learn: bool = False\n",
    "        ) -> None:\n",
    "        super().__init__(model, loss, optimizer, device, file_name, force_learn)\n",
    "\n",
    "    def get_output_from_batch(self, batch):\n",
    "        x, _, _ = batch\n",
    "        x = x.to(self.device)\n",
    "        _, output = self.model(x)\n",
    "        return x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_size = 64\n",
    "mode = DataMode.COLOR\n",
    "trainset = StackedMNIST(train=True, mode=mode)\n",
    "testset = StackedMNIST(train=False, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = StackedAE(latent_space_size=latent_space_size)\n",
    "Trainer = ModelTrainer(\n",
    "    model=AE,\n",
    "    loss = loss,\n",
    "    optimizer=None,\n",
    "    file_name=model_file,\n",
    "    force_learn=False\n",
    ")\n",
    "# Trainer = Trainer.load_trainer(trainer_file)\n",
    "VerifNet = VerificationNet(file_name='models/verification_model_torch_ok_copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reconstructed_img(dataset, batch_size: int = 25, print_original_image: bool = False):\n",
    "    data = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    imgs, _, labels = next(iter(data))\n",
    "    _, imgs_pred = AE(imgs.to(device))\n",
    "    labels = labels.detach().numpy()\n",
    "    imgs_pred = imgs_pred.permute(0, 2, 3, 1).to(\"cpu\").detach().numpy()\n",
    "\n",
    "    tile_tv_images(images=imgs_pred, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show STACK basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.print_reconstructed_img(testset, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_reconstructed_img(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show STACK GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10_000\n",
    "z_noise = nn.Softmax(dim=2)(torch.rand(batch_size, 3, latent_space_size)).to(device)\n",
    "\n",
    "x_hats = []\n",
    "for channel in range(3):\n",
    "    x_hat = AE.decoder(z_noise[:,[channel],:].view(-1, latent_space_size))\n",
    "    x_hats.append(x_hat)\n",
    "\n",
    "img_gens = torch.cat(x_hats, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = VerifNet.check_class_coverage(data=img_gens, tolerance=.5)\n",
    "pred, acc = VerifNet.check_predictability(data=img_gens, tolerance=.5)\n",
    "print(f\"Coverage: {100*cov:.2f}%\")\n",
    "print(f\"Predictability: {100*pred:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_labels, beliefs = VerifNet.predict(img_gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_beliefs = np.flip(np.argsort(beliefs))[:16]\n",
    "imgs_gen_to_plot = img_gens.permute(0, 2, 3, 1).to(\"cpu\").detach().numpy()[arg_beliefs]\n",
    "\n",
    "tile_tv_images(images=imgs_gen_to_plot, labels=noise_labels[arg_beliefs].astype(np.intc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show STACK MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_size = 64\n",
    "mode = DataMode.COLOR\n",
    "trainset = StackedMNIST(train=True, mode=mode | DataMode.MISSING)\n",
    "testset = StackedMNIST(train=False, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = StackedAE(latent_space_size=latent_space_size)\n",
    "Trainer = ModelTrainer(\n",
    "    model=AE,\n",
    "    loss = None,\n",
    "    optimizer=None,\n",
    "    file_name=\"models/ae-anom\",\n",
    "    force_learn=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(trainset, shuffle=True, batch_size=2048)\n",
    "test_set = DataLoader(testset, shuffle=True, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.print_class_coverage_and_predictability(VerifNet, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.print_class_coverage_and_predictability(VerifNet, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(trainset, shuffle=True, batch_size=10000)\n",
    "imgs, _, labels = next(iter(data))\n",
    "\n",
    "labels = labels.detach().numpy()\n",
    "_, preds = AE(imgs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hat, beliefs = VerifNet.predict(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argwhere(labels_hat==8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_accuracies(X, X_hat, labels):\n",
    "    class_accuracies = [[] for _ in range(10)]\n",
    "\n",
    "    for x, x_hat, label in zip(X, X_hat, labels):\n",
    "        class_accuracies[label].append(loss(x, x_hat))\n",
    "\n",
    "    mean_accuracy = [np.mean(acc) for acc in class_accuracies]\n",
    "    std_accuracy = [np.std(acc) for acc in class_accuracies]\n",
    "\n",
    "    return mean_accuracy, std_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(testset, shuffle=True, batch_size=10_000)\n",
    "imgs, _, labels = next(iter(data))\n",
    "\n",
    "labels = labels.detach().numpy()\n",
    "_, preds = AE(imgs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imgs.to(device)\n",
    "X_hat = preds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accuracies = [[] for _ in range(1000)]\n",
    "losses = []\n",
    "for x, x_hat, label in zip(X, X_hat, labels):\n",
    "    class_accuracies[label].append(loss(x, x_hat).item())\n",
    "    losses.append(loss(x, x_hat).item())\n",
    "    \n",
    "mean_accuracy = [np.mean(acc) for acc in class_accuracies]\n",
    "std_accuracy = [np.std(acc) for acc in class_accuracies]\n",
    "losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorbar_classification(np.arange(1000), mean_accuracy=mean_accuracy, std_accuracy=std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_anomalous(15, losses, preds.permute(0, 2, 3, 1).to(\"cpu\").detach().numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mean_anomalous(15, mean_accuracy, preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
