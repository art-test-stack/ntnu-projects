{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import top_anomalous, top_mean_anomalous, errorbar_classification\n",
    "from util import tile_tv_images\n",
    "\n",
    "from variational_autoencoder import VariationalAutoEncoder\n",
    "from verification_net import VerificationNet\n",
    "from model_trainer import ModelTrainer\n",
    "from stacked_mnist import StackedMNIST, DataMode\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "torch.mps.set_per_process_memory_fraction(0.)\n",
    "\n",
    "trainer_file = Path(\"trainers/vae-anom.pkl\")\n",
    "model_file = Path(\"models/vae-anom\")\n",
    "\n",
    "mode = DataMode.MONO | DataMode.BINARY\n",
    "\n",
    "trainset = StackedMNIST(train=True, mode=mode | DataMode.MISSING)\n",
    "testset = StackedMNIST(train=False, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderTrainer(ModelTrainer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            loss, \n",
    "            optimizer,\n",
    "            device = torch.device(\"mps\"),\n",
    "            file_name: str | Path = model_file, \n",
    "            force_learn: bool = False\n",
    "        ) -> None:\n",
    "        super().__init__(model, loss, optimizer, device, file_name, force_learn)\n",
    "\n",
    "    def get_output_from_batch(self, batch):\n",
    "        x, _, _ = batch\n",
    "        x = x.to(self.device)\n",
    "        (mu, log_var), x_hat = self.model(x)\n",
    "        return (x_hat, x), (mu, log_var)\n",
    "    \n",
    "def loss(X, params):\n",
    "    x_hat, x = X\n",
    "    mu, log_var = params\n",
    "    BCE = F.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "    KLD = torch.mean(- 0.5 * torch.mean(1 + log_var - mu.pow(2) - torch.exp(log_var), axis=1))\n",
    "\n",
    "    return BCE + .02 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = VariationalAutoEncoder()\n",
    "\n",
    "opt = optim.Adam(VAE.parameters(), lr=1e-6)\n",
    "\n",
    "Trainer = VariationalAutoEncoderTrainer(\n",
    "        model=VAE, \n",
    "        loss=loss, \n",
    "        optimizer=opt, \n",
    "        file_name=model_file,\n",
    "        force_learn=False\n",
    "    )\n",
    "\n",
    "batch_size = 1\n",
    "data = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "x, _, labels = next(iter(data))\n",
    "\n",
    "Trainer.optimizer = optim.Adam(VAE.parameters(), lr=5e-4)\n",
    "\n",
    "Trainer.train(\n",
    "        trainset=trainset, \n",
    "        valset=testset, \n",
    "        epochs=100, \n",
    "        batch_size=2048\n",
    "    )\n",
    "\n",
    "Trainer.load_trainer(trainer_file=trainer_file)\n",
    "\n",
    "plt.plot(Trainer.losses, label=\"train loss\")\n",
    "plt.plot(Trainer.val_losses, label=\"val loss\")\n",
    "plt.legend()\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset, shuffle=True, batch_size=2048)\n",
    "test_loader = DataLoader(dataset=testset, shuffle=True, batch_size=2048)\n",
    "\n",
    "Trainer.print_reconstructed_img(trainset, batch_size=16)\n",
    "Trainer.print_reconstructed_img(testset, batch_size=16)\n",
    "\n",
    "VerifNet = VerificationNet(force_learn=False, file_name='models/verification_model_torch_ok_copy')\n",
    "\n",
    "Trainer.print_class_coverage_and_predictability(VerifNet, trainset, batch_size=10_000)\n",
    "Trainer.print_class_coverage_and_predictability(VerifNet, testset, batch_size=10_000)\n",
    "\n",
    "data = DataLoader(testset, shuffle=True, batch_size=10000)\n",
    "imgs, _, labels = next(iter(data))\n",
    "\n",
    "labels = labels.detach().numpy()\n",
    "imgs = imgs.to(device)\n",
    "_, preds = VAE(imgs)\n",
    "\n",
    "nb_samples = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "class_accuracies = [[] for _ in range(10)]\n",
    "probas = []\n",
    "for x, label in tqdm(zip(imgs, labels)):\n",
    "    x = x.view(1, 1, 28, 28)\n",
    "    mu, log_var = VAE.encoder(x)\n",
    "\n",
    "    std = torch.sqrt(torch.exp(log_var)).to(device)\n",
    "    eps = torch.randn_like(std.expand(nb_samples,64)).to(device)\n",
    "    z = mu + eps * std\n",
    "    x_hat = VAE.decoder(z)\n",
    "\n",
    "    x = x.expand(nb_samples, 1, 28, 28)\n",
    "    class_accuracies[label].append((-F.binary_cross_entropy(x_hat, x)).exp().mean().cpu().detach().numpy())\n",
    "    probas.append((-F.binary_cross_entropy(x_hat, x)).exp().mean().cpu().detach().numpy())\n",
    "    \n",
    "mean_accuracy = [np.mean(acc) for acc in class_accuracies]\n",
    "std_accuracy = [np.std(acc) for acc in class_accuracies]\n",
    "\n",
    "probas = np.array(probas)\n",
    "\n",
    "errorbar_classification(np.arange(10), mean_accuracy=mean_accuracy, std_accuracy=std_accuracy, label=\"p(x)\")\n",
    "\n",
    "def top_anomalous_vae(k, losses, preds, labels, is_bottom = True):\n",
    "    idx = np.argsort(losses)[:k] if is_bottom else np.flip(np.argsort(losses))[:k]\n",
    "\n",
    "    new_labels = [ \n",
    "        str(f\"p(x={labels[_id]}): {int(losses[_id] * 1e2) / 1e2}\") for _id in idx\n",
    "        ]\n",
    "    tile_tv_images(images=preds[idx], labels=new_labels)\n",
    "\n",
    "top_anomalous_vae(15, probas, preds.cpu().detach().numpy().reshape(-1, 28, 28), labels)\n",
    "top_anomalous_vae(15, probas, imgs.cpu().detach().numpy().reshape(-1, 28, 28), labels)\n",
    "\n",
    "x_values = np.argsort(mean_accuracy)[:9]\n",
    "is_color = preds.shape[1] == 3\n",
    "\n",
    "new_labels = [ \n",
    "    str(f\"p(x={x_value}) = {int(mean_accuracy[x_value] * 1e4) / 1e3}e-1\") for x_value in x_values\n",
    "    ]\n",
    "np_imgs_predicted = preds.reshape(-1, 28, 28).cpu().detach().numpy() if not is_color else preds.permute(0,2,3,1).cpu().detach().numpy()\n",
    "\n",
    "anomalies_pred = np.array([ np_imgs_predicted[np.argwhere(labels == x_value)[:,0]][0] for x_value in x_values ])\n",
    "\n",
    "tile_tv_images(images=anomalies_pred, labels=new_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
