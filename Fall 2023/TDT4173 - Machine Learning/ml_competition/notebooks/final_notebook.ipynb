{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team name: Bsahtek le ML\n",
    "\n",
    "Students: \n",
    "- TESTARD Arthur: 105022\n",
    "- VERDON Valentin: 106002\n",
    "- VERDIER Nahel: 105247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "## Final Notebook of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "# \n",
    "0. [Imports and data initialization](#0-imports-and-data-initialization)\n",
    "1. [Some Data analysis](#1-data-analysis)\n",
    "    1. [Global Analysis](#11-global-analysis)\n",
    "    2. [Nan study](#12-nan-study)\n",
    "    3. [Study of the target](#13-study-of-the-target)\n",
    "    4. [Constant columns study](#15-constant-columns-study)\n",
    "    5. [Correlation study](#16-correlation-study)\n",
    "    <!-- 1. [Sub paragraph](#2-research-leads) -->\n",
    "2. [Research leads](#2-research-leads)\n",
    "    1. [Signal analysis](#21-signal-analysis)\n",
    "3. [Preprocessing](#3-preprocessing)\n",
    "    1. [Management of Nan values](#31-management-of-nan-values)\n",
    "    2. [Constant columns management](#32-constant-columns-management)\n",
    "    3. [Normalization](#33-normalization)\n",
    "    4. [Feature engineering](#34-feature-engineering)\n",
    "    5. [Management of constant parts of pv_measurement on B and C](#35-management-of-constant-parts-of-pv_measurement-on-b-and-c)\n",
    "    6. [Interpolation of the output values](#36-interpolation-of-the-output-values)\n",
    "    7. [Input reshaping](#37-input-reshaping)\n",
    "    8. [Pre-processing function](#38-preprocessing-function)\n",
    "4. [Pre-processing ideas that didn't work](#4-pre-processing-ideas-that-didnt-work)\n",
    "    1. [Polynomial features](#41-polynomial-features)\n",
    "    2. [Mean of features](#42-mean-of-features)\n",
    "5. [Separation of training and test sets](#5-separation-of-training-and-test-sets)\n",
    "    1. [Split training and test sets on estimated set dates](#51-split-training-and-test-sets-on-estimated-set-dates)\n",
    "    2. [Split training and test sets on prediction dates range and estimated set](#52-split-training-and-test-sets-on-prediction-dates-range-and-estimated-set)\n",
    "6. [Models](#6-models)\n",
    "    1. [Models performance comparison](#61-models-performance-comparison)\n",
    "    2. [XGBoost hyper-parameters](#62-xgboost-hyper-parameters)\n",
    "    3. [Hyper-parameters selction](#63-post-processing)\n",
    "    4. [Some other models which did not succeed](#64-some-other-models-which-did-not-succeed)\n",
    "7. [Areas for improvement](#7-areas-for-improvement)\n",
    "8. [Conclusion](#conclusion)\n",
    "<!-- 7. [Another paragraph](#part7) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "\n",
    "import tensorflow as tf\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFolder:\n",
    "    def __init__(self, folder_name: str):\n",
    "        self.folder_name: str\n",
    "        self.X_test_estimated: str = f\"{folder_name}/X_test_estimated.parquet\"\n",
    "        self.X_train_estimated: str = f\"{folder_name}/X_train_estimated.parquet\"\n",
    "        self.X_train_observed: str = f\"{folder_name}/X_train_observed.parquet\"\n",
    "        self.train_targets: str | None = f\"{folder_name}/train_targets.parquet\"\n",
    "\n",
    "A = DataFolder(folder_name='A')\n",
    "B = DataFolder(folder_name='B')\n",
    "C = DataFolder(folder_name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(diff_path: str = '../'):\n",
    "    train_a = pd.read_parquet(diff_path + A.train_targets)\n",
    "    train_b = pd.read_parquet(diff_path + B.train_targets)\n",
    "    train_c = pd.read_parquet(diff_path + C.train_targets)\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(diff_path + A.X_train_estimated)\n",
    "    X_train_estimated_b = pd.read_parquet(diff_path + B.X_train_estimated)\n",
    "    X_train_estimated_c = pd.read_parquet(diff_path + C.X_train_estimated)\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(diff_path + A.X_train_observed)\n",
    "    X_train_observed_b = pd.read_parquet(diff_path + B.X_train_observed)\n",
    "    X_train_observed_c = pd.read_parquet(diff_path + C.X_train_observed)\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(diff_path + A.X_test_estimated)\n",
    "    X_test_estimated_b = pd.read_parquet(diff_path + B.X_test_estimated)\n",
    "    X_test_estimated_c = pd.read_parquet(diff_path + C.X_test_estimated)\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Global analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll get to grips with the data and begin to understand its structure, what it's all about, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files dimensions:\")\n",
    "print(f\"train_a: {train_a.shape}, train_b:, {train_b.shape}, train_c: {train_c.shape}\")\n",
    "print(f\"X_train_estimated_a:, {X_train_estimated_a.shape}, X_train_estimated_b: {X_train_estimated_b.shape}, X_train_estimated_c: {X_train_estimated_c.shape}\")\n",
    "print(f\"X_train_observed_a: {X_train_observed_a.shape}, X_train_observed_b: {X_train_observed_b.shape}, X_train_observed_c: {X_train_observed_c.shape}\")\n",
    "print(f\"X_train_estimated_a: {X_train_estimated_a.shape}, X_train_estimated_b: {X_train_estimated_b.shape}, X_train_estimated_c: {X_train_estimated_c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that `X_train_estimated` and `X_test_estimated` has one more column than `X_test_observed` which is `date_calc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the variable types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "for type in X_train_estimated_a.dtypes.values:\n",
    "    if type not in types: types.append(type)\n",
    "\n",
    "for type in X_train_estimated_b.dtypes.values:\n",
    "    if type not in types: types.append(type)\n",
    "\n",
    "for type in X_train_estimated_c.dtypes.values:\n",
    "    if type not in types: types.append(type)\n",
    "\n",
    "print(\"Our data types are only that kind:\", types, \"(datetime and float)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the start and end dates of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_observed_a:\", X_train_observed_a['date_forecast'].min(), X_train_observed_a['date_forecast'].max())\n",
    "print(\"X_train_estimated_a:\", X_train_estimated_a['date_forecast'].min(), X_train_estimated_a['date_forecast'].max())\n",
    "print(\"X_train_observed_b:\",X_train_observed_b['date_forecast'].min(), X_train_observed_b['date_forecast'].max())\n",
    "print(\"X_train_estimated_b:\", X_train_estimated_b['date_forecast'].min(), X_train_estimated_b['date_forecast'].max())\n",
    "print(\"X_train_observed_c:\", X_train_observed_c['date_forecast'].min(), X_train_observed_c['date_forecast'].max())\n",
    "print(\"X_train_estimated_c:\", X_train_estimated_c['date_forecast'].min(), X_train_estimated_c['date_forecast'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Nan study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll see how our missing data is distributed, so that we can deal with this problem later. Gives the proportion of missing values for each column in the DataFrame, sorted from highest to lowest, as a percentage of the total number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just look at `X_train_observed_a` as an example in order to earn so time on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a.isna().sum().sort_values(ascending=False)/(len(X_train_estimated_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the columns with NAN values are `snow_density`, `ceiling_height_agl` and `cloud_base_agl`. `snow_density` is empty at 90% while the others are empty at 5 to 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Study of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study the target to see how it evolves over time. If there is information to be deduced that our model will need to understand and use to make its predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show variables for 3 locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the variable pv_measurement as a functio of time\n",
    "fig, axs = plt.subplots(3, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "train_a[['time','pv_measurement']].set_index('time').plot(ax=axs[0], title='pv_measurement on location A')\n",
    "train_b[['time','pv_measurement']].set_index('time').plot(ax=axs[1], title='pv_measurement on location B')\n",
    "train_c[['time','pv_measurement']].set_index('time').plot(ax=axs[2], title='pv_measurement on location C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data show periodicities. In section 2.1.Signal analysis we have noticed 3 main frequencies: the year, the day and 12h.\n",
    "\n",
    "Our variable also has gaps or constant parts in certain areas that need to be explored in detail.\n",
    "\n",
    "However, our variable varies a lot according to the weather, hence the need for a Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = train_a.rename(columns={'time': 'date_forecast'})\n",
    "train_b = train_b.rename(columns={'time': 'date_forecast'})\n",
    "train_c = train_c.rename(columns={'time': 'date_forecast'})\n",
    "\n",
    "merged_df_a = pd.merge(X_train_observed_a, train_a, on='date_forecast', how='inner')\n",
    "merged_df_b = pd.merge(X_train_observed_b, train_b, on='date_forecast', how='inner')\n",
    "merged_df_c = pd.merge(X_train_observed_c, train_c, on='date_forecast', how='inner')\n",
    "\n",
    "# Add prefixes to column names during concatenation\n",
    "merged_df_a.columns = 'a_' + merged_df_a.columns\n",
    "merged_df_b.columns = 'b_' + merged_df_b.columns\n",
    "merged_df_c.columns = 'c_' + merged_df_c.columns\n",
    "\n",
    "merged_df_a = merged_df_a.rename(columns={'a_date_forecast': 'date_forecast'})\n",
    "merged_df_b = merged_df_b.rename(columns={'b_date_forecast': 'date_forecast'})\n",
    "merged_df_c = merged_df_c.rename(columns={'c_date_forecast': 'date_forecast'})\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "aux =  pd.merge(merged_df_a, merged_df_b, on='date_forecast', how='inner')\n",
    "total = pd.merge(aux, merged_df_c, on='date_forecast', how='inner')\n",
    "\n",
    "print('correlation between A and B of pv_measurement:',total['a_pv_measurement'].corr(total['b_pv_measurement']))\n",
    "print('corrélation between A and C of pv_measurement:',total['a_pv_measurement'].corr(total['c_pv_measurement']))\n",
    "print('corrélation between B and C of pv_measurement:',total['b_pv_measurement'].corr(total['c_pv_measurement']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Inputs comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Sun-related variables|Snow-related variables|Atmospheric pressure variables|Rain variables|Wind variables|Cloud-related variables|Visibility and altitude variables|Variables related to diffuse and direct light|Day/night and shade variable|\n",
    "|--- |:-: |:-: |:-: |:-: |:-: |:-: |:-:   |--:   |\n",
    "|`clear_sky_energy_1h:J`|`fresh_snow_12h:cm`|`msl_pressure:hPa`|`precip_5min:mm`|`wind_speed_10m:ms`|`effective_cloud_cover:p`|`ceiling_height_agl:m`|`diffuse_rad:W`|`is_day:idx`|\n",
    "|`clear_sky_rad:W`|`fresh_snow_1h:cm`|`pressure_100m:hPa`|`rain_water:kgm2`|`wind_speed_u_10m:ms`|`total_cloud_cover:p`|`elevation:m`|`diffuse_rad_1h:J`|`is_in_shadow:idx`|\n",
    "|`sun_azimuth:d`|`fresh_snow_24h:cm`|`pressure_50m:hPa`|`prob_rime:p`|`wind_speed_v_10m:ms`|`relative_humidity_1000hPa:p`|`visibility:m`|`direct_rad:W`||\n",
    "|`sun_elevation:d`|`fresh_snow_3h:cm`|`sfc_pressure:hPa`|`precip_type_5min:idx`||`absolute_humidity_2m:gm3`||`direct_rad_1h:D`||\n",
    "|`sun_elevation:d`|`fresh_snow_6h:cm`|`t_1000hPa:K`|`dew_or_rime`||`air_density_2m:kgm3`||||\n",
    "||`snow_density:kgm3`|`wind_speed_w_1000hPa:ms`|`dew_point_2m`||`cloud_base_agl:m`||||\n",
    "||`snow_depth:cm`||||||||\n",
    "||`snow_drift:idx`||||||||\n",
    "||`snow_melt_10min:mm`||||||||\n",
    "||`snow_water:kgm2`||||||||\n",
    "||`super_cooled_liquid_water:kgm2`||||||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_a = MinMaxScaler()\n",
    "scaler_b = MinMaxScaler()\n",
    "scaler_c = MinMaxScaler()\n",
    "\n",
    "X_a_to_analyse = X_train_observed_a.drop(columns='date_forecast', inplace=False)\n",
    "X_b_to_analyse = X_train_observed_b.drop(columns='date_forecast', inplace=False)\n",
    "X_c_to_analyse = X_train_observed_c.drop(columns='date_forecast', inplace=False)\n",
    "\n",
    "columns_match_already_done = []\n",
    "X_a_normed = scaler_a.fit_transform(X_a_to_analyse)\n",
    "X_b_normed = scaler_b.fit_transform(X_b_to_analyse)\n",
    "X_c_normed = scaler_c.fit_transform(X_c_to_analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot here the columns which put together seems to be strongly linked (like we can write a relation such as $f(x_1) = x_2$, with $f$ a usual function, or a combination of it).\n",
    "\n",
    "If we look to every one of these combinations, we can notice that the link is pretty obvious. For example data about sun, such the energy or it position in the sky, are strongly linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(X_a_to_analyse.columns.values)\n",
    "columns_to_plot = [ \n",
    "    (\"dew_point_2m:K\",\"absolute_humidity_2m:gm3\"),\n",
    "    (\"msl_pressure:hPa\",\"pressure_100m:hPa\"),\n",
    "    (\"msl_pressure:hPa\",\"sfc_pressure:hPa\"),\n",
    "    (\"pressure_100m:hPa\",\"sfc_pressure:hPa\"),\n",
    "    (\"t_1000hPa:K\", \"absolute_humidity_2m:gm3\"),\n",
    "    (\"dew_point_2m:K\", \"t_1000hPa:K\"),\n",
    "    (\"diffuse_rad:W\", \"sun_azimuth:d\"),\n",
    "    (\"diffuse_rad_1h:J\",\"sun_azimuth:d\"),\n",
    "    (\"direct_rad:W\", \"direct_rad_1h:J\"),\n",
    "    (\"direct_rad:W\", \"sun_azimuth:d\"),\n",
    "    (\"direct_rad_1h:J\", \"sun_azimuth:d\"),\n",
    "    (\"sun_elevation:d\",\"sun_azimuth:d\"),\n",
    "]\n",
    "\n",
    "for c1, c2 in columns_to_plot:\n",
    "    c1_index = columns.index(c1)\n",
    "    c2_index = columns.index(c2)\n",
    "    plt.scatter(X_a_normed[:, c2_index], X_a_normed[:, c1_index], alpha=1, label='A position')\n",
    "    plt.scatter(X_b_normed[:, c2_index], X_b_normed[:, c1_index], alpha=0.1, label='B position')\n",
    "    plt.scatter(X_c_normed[:, c2_index], X_c_normed[:, c1_index], alpha=0.01, label='C position')\n",
    "    plt.xlabel(c1)\n",
    "    plt.ylabel(c2)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title(f\"{c2} = f({c1})\")\n",
    "    plt.show()\n",
    "    columns_match_already_done.append((c1, c2))\n",
    "    columns_match_already_done.append((c2, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Constant columns study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to determine here, which columns are constants, so useless for our future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "print(X_train_observed_a.std()[X_train_observed_a.std() == 0].keys(), X_train_observed_b.std()[X_train_observed_b.std() == 0].keys(), X_train_observed_c.std()[X_train_observed_c.std() == 0].keys())\n",
    "print(X_train_estimated_a.std()[X_train_estimated_a.std() == 0].keys(), X_train_estimated_b.std()[X_train_estimated_b.std() == 0].keys(), X_train_estimated_c.std()[X_train_estimated_c.std() == 0].keys())\n",
    "print(X_test_estimated_a.std()[X_test_estimated_a.std() == 0].keys(), X_test_estimated_b.std()[X_test_estimated_b.std() == 0].keys(), X_test_estimated_c.std()[X_test_estimated_c.std() == 0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say three things about this result. First, we can see that the `elevation:m` is like a parameters which tell us the position of the solar panel. Secondly we see that our weather estimation do not predict `wind_speed_w_1000hPa:m`, so we need to consider it in our model. Last thing is that we could ajust our model with the snow parameters, because it is constant in test estimated sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Correlation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the correlation matrices, we can begin to study the relationships between the different variables in order to potentially distinguish groups, or other features of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corr_matrix(data_frames, figsize=(20,20), annot=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(data_frames.corr(method='pearson'), annot=annot, cmap=plt.cm.Reds)\n",
    "    \n",
    "keys = [1, 2, 3]\n",
    "\n",
    "frames_train_estimated = [X_train_estimated_a.drop(columns=[\"date_calc\"]), X_train_observed_a, X_test_estimated_a.drop(columns=[\"date_calc\"])]\n",
    "X_frames_a = pd.concat(frames_train_estimated, keys=keys)\n",
    "X_frames_a.reset_index(level=0, inplace=True, names='frame_type')\n",
    "\n",
    "train_a = train_a.rename(columns={'time': 'date_forecast'})\n",
    "X_y_a = X_frames_a.merge(train_a, on='date_forecast', how='inner')\n",
    "X_y_a.reset_index(drop=True, inplace=True)\n",
    "\n",
    "build_corr_matrix(X_y_a, figsize=(50,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most correlated variables for pv_measurement are :\n",
    "\n",
    "*   `clear_sky_energy_1h:J`: clear sky energy of previous time period, available up to 24h [J/m2]\n",
    "*   `clear_sky_rad:W`: clear sky radiation flux [W/m2]\n",
    "*   `diffuse_rad:W`: diffuse radiation flux [W/m2]\n",
    "*   `diffuse_rad_1h:J`: accumulated diffuse radiation of previous time period, available up to 24h [J/m2]\n",
    "*   `direct_rad:W`: direct radiation flux [W/m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_train_estimated = [X_train_estimated_b.drop(columns=[\"date_calc\"]), X_train_observed_b, X_test_estimated_b.drop(columns=[\"date_calc\"])]\n",
    "X_frames_b = pd.concat(frames_train_estimated, keys=keys)\n",
    "X_frames_b.reset_index(level=0, inplace=True, names='frame_type')\n",
    "\n",
    "train_b = train_b.rename(columns={'time': 'date_forecast'})\n",
    "X_y_b = X_frames_b.merge(train_b.dropna(), on='date_forecast', how='inner')\n",
    "X_y_b.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frames_train_estimated = [X_train_estimated_c.drop(columns=[\"date_calc\"]), X_train_observed_c, X_test_estimated_c.drop(columns=[\"date_calc\"])]\n",
    "X_frames_c = pd.concat(frames_train_estimated, keys=keys)\n",
    "X_frames_c.reset_index(level=0, inplace=True, names='frame_type')\n",
    "\n",
    "train_c = train_c.rename(columns={'time': 'date_forecast'})\n",
    "X_y_c = X_frames_c.merge(train_c.dropna(), on='date_forecast', how='inner')\n",
    "X_y_c.reset_index(drop=True, inplace=True)\n",
    "\n",
    "frames_location= [X_y_a, X_y_b, X_y_c]\n",
    "X_y = pd.concat(frames_location, keys=keys)\n",
    "X_y.reset_index(level=0, inplace=True, names='location')\n",
    "\n",
    "build_corr_matrix(X_y, figsize=(50,50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_train_estimated = [X_train_estimated_a, X_train_estimated_b, X_train_estimated_c]\n",
    "X_train_estimated = pd.concat(frames_train_estimated, keys=keys)\n",
    "X_train_estimated.reset_index(level=0, inplace=True, names='location')\n",
    "\n",
    "frames_train_observed = [X_train_observed_a, X_train_observed_b, X_train_observed_c]\n",
    "X_train_observed = pd.concat(frames_train_observed, keys=keys)\n",
    "X_train_observed.reset_index(level=0, inplace=True, names='location')\n",
    "\n",
    "frames_test_estimated = [X_test_estimated_a, X_test_estimated_b, X_test_estimated_c]\n",
    "X_test_estimated = pd.concat(frames_test_estimated, keys=keys)\n",
    "X_test_estimated.reset_index(level=0, inplace=True, names='location')\n",
    "\n",
    "X_train_estimated_merged = pd.concat(frames_train_estimated, axis=1)\n",
    "X_train_estimated_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_observed_merged = pd.concat(frames_train_observed, axis=1)\n",
    "X_train_observed_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_test_estimated_merged = pd.concat(frames_test_estimated, axis=1)\n",
    "X_test_estimated_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "build_corr_matrix(X_train_estimated_merged, figsize=(100,100), annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research leads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Signal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data were presenting some periodicities, intuitively, one of our first idea were to analyse the different signals we have, starting by our target, `pv_measurement`. However, as we can see on the following plot, the data is not completly cleared, specially on B and C. We will come back to this point in Preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "def filter_dates_when_constants(df, date_c = 'time', y = 'pv_measurement', delta = { 'days': 3 }):\n",
    "    df = df.copy()\n",
    "    mask_y_change = df[y] != df[y].shift(1)\n",
    "\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    constant_periods = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if not mask_y_change[index]:\n",
    "            if start_date is None:\n",
    "                start_date = row[date_c]\n",
    "            end_date = row[date_c]\n",
    "        else:\n",
    "            if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "                constant_periods.append((start_date, end_date))\n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "    if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "        constant_periods.append((start_date, end_date))\n",
    "    return constant_periods\n",
    "\n",
    "def delete_date_range_from_df(df, dates, date_c = 'time'):\n",
    "    df = df.copy()\n",
    "    c = 0\n",
    "    for start_date, end_date in dates:\n",
    "        mask = (df[date_c] >= start_date) & (df[date_c] < end_date)\n",
    "        df = df[~mask]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "delta = { 'hours': 12 * 4}\n",
    "train_a = delete_date_range_from_df(train_a, filter_dates_when_constants(train_a, delta=delta))\n",
    "train_b = delete_date_range_from_df(train_b, filter_dates_when_constants(train_b, delta=delta))\n",
    "train_c = delete_date_range_from_df(train_c, filter_dates_when_constants(train_c, delta=delta))\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "train_a[['time','pv_measurement']].set_index('time').plot(ax=axs[0], title='pv_measurement on location A')\n",
    "train_b[['time','pv_measurement']].set_index('time').plot(ax=axs[1], title='pv_measurement on location B')\n",
    "train_c[['time','pv_measurement']].set_index('time').plot(ax=axs[2], title='pv_measurement on location C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we then tryied to analys this signal with basic components such as Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_transforms(train):\n",
    "    y = train[\"pv_measurement\"].dropna().values\n",
    "    time_diff = train[\"time\"].diff().mean().total_seconds()\n",
    "    sampling_rate = 1 / time_diff\n",
    "\n",
    "    n = len(y)\n",
    "    freq = np.fft.fftfreq(n, 1 / sampling_rate)\n",
    "    fft_y = np.fft.fft(y)\n",
    "    amp_fft_y = np.abs(fft_y)\n",
    "    phase = np.angle(fft_y)\n",
    "    return freq, fft_y, amp_fft_y, phase, sampling_rate\n",
    "\n",
    "trains = { 'A': train_a.dropna(subset='pv_measurement'), 'B': train_b.dropna(subset='pv_measurement'), 'C': train_c.dropna(subset='pv_measurement') }\n",
    "locations = trains.keys()\n",
    "\n",
    "freqs, fft_ys, amp_fft_ys, phases, sampling_rates = [\n",
    "    { \n",
    "        loc: get_fft_transforms(trains[loc])[k] for loc in locations \n",
    "    } \n",
    "    for k in range(5)]\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], amp_fft_ys[loc][:len(freqs[loc])//2])\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], 20 * np.log10(amp_fft_ys[loc][:len(freqs[loc])//2]))\n",
    "    plt.title(f\"Spectrum in magnitude of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked on the most important frequencies in those spectrums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_peak_frequencies(amp_fft_y, freq, threshold, loc):\n",
    "    peaks, _ = scipy.signal.find_peaks(amp_fft_y[:len(amp_fft_y)//2], height=threshold)\n",
    "    peak_frequencies = freq[:len(freq)//2][peaks]\n",
    "\n",
    "    period_size = int(1/peak_frequencies[0])\n",
    "    continuous_component = np.mean(trains[loc][\"pv_measurement\"].dropna().values[:period_size])\n",
    "\n",
    "    print(\"Location:\", loc)\n",
    "    print(f'Most important periods (in days): \\n{1 / peak_frequencies / 3600 / 24}')\n",
    "    print(f'Value of the continous component: {continuous_component}\\n\\n')\n",
    "\n",
    "thresholds = { 'A': .5e7, 'B': .5e6, 'C': .25e6 }\n",
    "for loc in locations:\n",
    "    print_peak_frequencies(amp_fft_ys[loc], freqs[loc], thresholds[loc], loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First commentaries:\n",
    "\n",
    "We know from the analysis of the nan values that A got the most clean datas in term of `pv_measurement` values. So our analysis will mostly be based on what we see on A. We can notice 3 most important frequencies: one for the year, one for the day and one for a half-day (12 hours). If we look more on the frequency plot, we can notice a most little one frequency (that our threshold impeach us to read it on the last print). This seems to be a peak for a period of 8 hours, according to the code cell bellow.\n",
    "\n",
    "Because B and C are not much clean, we can suppose that the big differencies we found with A comes from the Nan values, which create some empty cells in these frames, which are compensated by increasing the frequency values. However, we did not pay attention to it much at first be because most of our analysis were based on A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_a_1 = np.min(np.where(freqs['A'] > .00003)) \n",
    "freq_a_2 = np.max(np.where(freqs['A'] < .00004))\n",
    "freq_arg = np.argmax(fft_ys['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2])\n",
    "1 / freqs['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2][freq_arg] / 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm what we sayied on B and C compared to A if we look on the differents sampling rates depending on the situation. Theorically, it should be close to one hour ($=3600$ seconds) because our values are measured every hours. But if we look on `1 / sampling_rates['B']` and `1 / sampling_rates['C']` we see that it's more than it for B and C locations. This comes from Nan values and confirms our point above.\n",
    "\n",
    "We can notice that `1 / sampling_rates['B']` is a bit bigger than an hour. We can explain it by the gap of one week between `X_train_observed_a` and `X_train_estimated_a`, which exists as well in `train_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / sampling_rates['A'] / 3600, 1 / sampling_rates['B'] / 3600, 1 / sampling_rates['C'] / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can try is to recalculate the model by the inverse of Fourier's transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fft_ys = { loc: np.fft.ifft(fft_ys[loc]) for loc in locations }\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(trains[loc]['pv_measurement'], label='real pv_measurement')\n",
    "    plt.plot(i_fft_ys[loc], label='ifft pv_measurement')\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a gap created in C but in facts its due to the index. \n",
    "\n",
    "Now the idea is to keep only the most important frequencies in order to have a model which can be written like this:\n",
    "$$y[n] = \\hat{y}[n] + r[n]$$\n",
    "\n",
    "where $n$ is the index of the output, $y[n]$ is the real value of `pv_measurement` at index $n$ (or time $t$), $\\hat{y}[n]$ is the value at index $n$ of the signal filtered predicted by signal analysis and $r[n]$ is the value at index $n$ of the noise created by mostly, the weather, from our inputs `X_train_estimated`, `X_train_observed`, etc. It would be design by a machine learning model. Actually we did not had the time to test this feature entirely, because of a lack of time our goals priotization. So, it is not entirely designed, but we will detail as far as we came to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': '2022-10-21', 'B': '2022-03-15', 'C': '2022-04-01'}\n",
    "# end_dates = { 'A': X_train_observed_a['date_forecast'].max(), 'B': X_train_observed_b['date_forecast'].max(), 'C': X_train_observed_c['date_forecast'].max() }\n",
    "\n",
    "start_dates = { 'A': '2020-10-21', 'B': '2020-03-15', 'C': '2020-04-01' }\n",
    "\n",
    "def get_model(fft_values, threshold=60, sample_rate=1):\n",
    "\n",
    "    n = len(fft_values)\n",
    "\n",
    "    frequencies = np.fft.fftfreq(n, 1 / sample_rate)\n",
    "    amplitudes = fft_values * (np.abs(fft_values) > threshold)\n",
    "    phases = np.angle(fft_values)\n",
    "    return {\"frequencies\": frequencies, \"amplitudes\": amplitudes, \"phases\": phases}\n",
    "\n",
    "def reconstruct_signal(model, duration,sample_rate):\n",
    "    frequencies = model[\"frequencies\"]\n",
    "    amplitudes = model[\"amplitudes\"]\n",
    "    phases = model[\"phases\"]\n",
    "\n",
    "    t = np.arange(0, duration, 1)\n",
    "    signal = np.zeros(len(t), dtype=np.complex128)\n",
    "    \n",
    "    for freq, amp, phase in tqdm(zip(frequencies, amplitudes, phases)):\n",
    "        signal += amp * np.exp(2j * np.pi * freq * t / sample_rate + phase)\n",
    "    return signal / len(frequencies)\n",
    "\n",
    "def get_thresholds_to_get_n_freq(signal, nb_freq, threshold, step):\n",
    "    assert step > 0\n",
    "    fft = np.fft.fft(signal)\n",
    "    abs_fft = np.abs(fft[:len(fft)//2])\n",
    "\n",
    "    freqs = [ f for f in abs_fft if f > threshold ]\n",
    "    threshold += step\n",
    "    while len(freqs) > nb_freq:\n",
    "        freqs = [ f for f in abs_fft if f > threshold ]\n",
    "        threshold += step\n",
    "    threshold = threshold if len(freqs) > 0 else threshold - step\n",
    "    return threshold\n",
    "\n",
    "def get_filtred_signal(signal, nb_freqs, sample_rate, nb_days_to_predict = 0, threshold = 0, scaler = StandardScaler):\n",
    "    scaler_pred = scaler\n",
    "    scaler = scaler()\n",
    "    Y_normed = scaler.fit_transform(np.array(signal['pv_measurement'].dropna()).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    threshold = get_thresholds_to_get_n_freq(signal=Y_normed, nb_freq=nb_freqs, threshold=0, step=.5)\n",
    "    model = get_model(fft_values=np.fft.fft(Y_normed), threshold=threshold, sample_rate=sample_rate)\n",
    "    pred_from_model_data = np.real(reconstruct_signal(model, duration=len(model[\"frequencies\"]) + nb_days_to_predict, sample_rate=sample_rate)) \n",
    "    scaler_pred = scaler_pred()\n",
    "    pred_normed = scaler_pred.fit_transform(pred_from_model_data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    Y_filtred = scaler.inverse_transform(pred_normed.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # If we want to filter negative values\n",
    "    Y_filtred[Y_filtred < 0] = 0\n",
    "    return Y_filtred\n",
    "\n",
    "nb_freqs = 2\n",
    "nb_days_to_predict = 0\n",
    "\n",
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "Y_filtred = { loc: get_filtred_signal(trains_on_dates[loc], nb_freqs, sampling_rates[loc], nb_days_to_predict=nb_days_to_predict) for loc in locations }\n",
    "\n",
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(Y_filtred[loc], color='b')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we explored different ways to design $\\hat{y}[n]$. The first one is a raw filter on the whole signal. This method were not much efficient. In our researchs we found `prophet`, a Python (and R) library which gives a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': X_train_estimated_a['date_forecast'].max(), 'B': X_train_estimated_b['date_forecast'].max(), 'C': X_train_estimated_c['date_forecast'].max() }\n",
    "trains = { loc: trains[loc][trains[loc]['time'] <= end_dates[loc]] for loc in locations}\n",
    "\n",
    "prophet_scalers = { loc: MinMaxScaler() for loc in locations }\n",
    "\n",
    "trains_raw_prophet = { loc: trains[loc].dropna(subset='pv_measurement').reset_index().rename(columns={'time': 'ds', 'pv_measurement': 'y'}) for loc in locations }\n",
    "\n",
    "trains_prophet = trains_raw_prophet\n",
    "for loc in locations:\n",
    "    trains_prophet[loc]['y'] = prophet_scalers[loc].fit_transform(np.array(trains_raw_prophet[loc]['y']).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "models_prophet = { loc: Prophet(changepoint_prior_scale=0.05) for loc in locations }\n",
    "predictions_prophet = {}\n",
    "forecast = {}\n",
    "\n",
    "for loc in locations:\n",
    "    models_prophet[loc].fit(trains_prophet[loc])\n",
    "    predictions_prophet[loc] = models_prophet[loc].make_future_dataframe(periods=66, freq='h')\n",
    "    forecast[loc] = models_prophet[loc].predict(predictions_prophet[loc])\n",
    "\n",
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(trains_prophet[loc]['y'], color='orange')\n",
    "    plt.plot(forecast[loc]['yhat'], color='b')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results, from prophet and the filter signal, are not really satisfying. Then came the idea, inpired by [this paper](https://peerj.com/preprints/3190.pdf), to see what's happen if we plot one signal for each hour (it would make $24 * 3 = 72$ models). We then first split our signals by hours and plot what we get with prophet prediction and our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL VALUES TO COMPILE\n",
    "hours = [ f\"0{h}\" if h < 10 else str(h) for h in range(24) ]\n",
    "\n",
    "nb_freqs = 1\n",
    "\n",
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "trains_on_hours = { loc: { h: trains_on_dates[loc][trains_on_dates[loc]['time'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "# MAYBE DROP NA ON B AND C\n",
    "Y_h_filtred = { loc: { h: get_filtred_signal(trains_on_hours[loc][h], nb_freqs, sampling_rates[loc] * 24, nb_days_to_predict=nb_days_to_predict, scaler=StandardScaler) for h in hours} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_on_hours[loc][h]['pv_measurement']), color='orange')\n",
    "        plt.plot(Y_h_filtred[loc][h], color='b')\n",
    "        plt.title(f\"Filtred signal at time h={h} for location {loc}\")\n",
    "        sp += 1\n",
    "\n",
    "def convert_hours_to_days(signal):\n",
    "    min_len = np.min([len(signal[h]) for h in hours ])\n",
    "    y_pred = []\n",
    "    for d in range(min_len):\n",
    "        for h in hours:\n",
    "            y_pred.append(signal[h][d])\n",
    "    return np.array(y_pred)\n",
    "\n",
    "reconstructed_train = { loc: convert_hours_to_days(Y_h_filtred[loc]) for loc in locations }\n",
    "\n",
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get here a far more satisfying result. There is a problem for B, we did not get why the curve does not go to 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = { loc: {} for loc in locations }\n",
    "trains_prophet_on_hours = { loc: { h: trains_prophet[loc][trains_prophet[loc]['ds'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(trains_prophet_on_hours[loc][h])) ] ]\n",
    "        trains_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': trains_prophet_on_hours[loc][h]['y']})\n",
    "\n",
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])\n",
    "\n",
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_prophet_on_hours[loc][h].reset_index()['y']), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1\n",
    "\n",
    "# Y_to_reconstruct = { loc: { h: prophet_scalers[loc].inverse_transform(np.array(forecast[loc][h]['yhat']).reshape(-1, 1)).reshape(-1) for h in hours } for loc in locations }\n",
    "Y_to_reconstruct = { loc: { h: np.array(forecast[loc][h]['yhat']) for h in hours } for loc in locations }\n",
    "reconstructed_train_prophet = { loc: convert_hours_to_days(Y_to_reconstruct[loc]) for loc in locations }\n",
    "\n",
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train_prophet[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_prophet[loc]['y']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are less satisfying than the precedent result. But let's see what happens if we try to predict the noise, $r[n]$, with Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prophet = { loc: { h: np.array(trains_on_hours[loc][h]['pv_measurement']) - Y_h_filtred[loc][h] for h in hours } for loc in locations }\n",
    "\n",
    "trains_noise_prophet_on_hours = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(noise_prophet[loc][h])) ] ]\n",
    "        trains_noise_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': noise_prophet[loc][h]})\n",
    "\n",
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_noise_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])\n",
    "\n",
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(noise_prophet[loc][h]), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this part, we did not found much successful results on it. The curve obtained were not that bad but we did not had much time to merge this with the biggest model. Maybe this approach were to much complicated as a first one and we should have focus on it later. From now, we consider it as a way to upgrade our current model that we are going to present in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our study, we explored many ways to pre-process our inputs. Some were compatible to each other, some were not. We are going to present in this part, all we did as pre-process and those we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Management of NAN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, we have 3 variables with Nan values: `snow_density:kgm3`,`ceiling_height_agl:m`,  `cloud_base_agl:m`.\n",
    "\n",
    "\n",
    "Since `snow_density:kgm3` is more than 95% empty, we're going to delete it because it's unusable.\n",
    "\n",
    "\n",
    "As for the other two, we've seen that they have missing values, probably due to measurement errors. We'll simply perform a **linear interpolation**, taking the average of the before and after values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gestion_nan(df):\n",
    "  df_copy = df.copy()\n",
    "  #delete of the snow density column\n",
    "  df_copy = df_copy.drop('snow_density:kgm3',axis=1)\n",
    "  # Approximation of the other two columns\n",
    "  df_copy['ceiling_height_agl:m'] = df_copy['ceiling_height_agl:m'].interpolate(method='linear', limit_direction='both')\n",
    "  df_copy['cloud_base_agl:m'] = df_copy['cloud_base_agl:m'].interpolate(method='linear', limit_direction='both')\n",
    "  return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Constant columns management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said we said in the Data analysis part, we decided to delete all the column which were constants in the test estimated sets, for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_a = X_test_estimated_a.std()[X_test_estimated_a.std() == 0].keys() \n",
    "columns_to_drop_b = X_test_estimated_b.std()[X_test_estimated_b.std() == 0].keys() \n",
    "columns_to_drop_c = X_test_estimated_c.std()[X_test_estimated_c.std() == 0].keys()\n",
    "\n",
    "def gestion_constant_columns(df, location):\n",
    "    columns_to_drop = columns_to_drop_a if location=='A' else columns_to_drop_b if location=='B' else columns_to_drop_c\n",
    "    return df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data have variable ranges that are quite different from one another. So we're going to do some normalization to solve this problem.\n",
    "\n",
    "To do this, we're going to use different normalizations, and when we train the model we'll see which normalization corresponds best to our dataset.\n",
    "\n",
    "The various standardizations used are as follows: StandardScaler, Normalizer, MinMaxScaler from the library sklearn.preprocessing\n",
    "\n",
    "Here's the typical function we'll use to normalize the data.\n",
    "\n",
    "Note: it adapts according to the train and test set, since the scaler must be the same for both stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_z_score_normalize_dataframe(df,return_scaler=False,scaler=None,fit=True):\n",
    "    \"\"\"\n",
    "    Normalizes a DataFrame using z-score normalization (mean and standard deviation) from Scikit-Learn.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The z-score normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if scaler == None :\n",
    "      # Create a StandardScaler instance\n",
    "      scaler = StandardScaler()\n",
    "\n",
    "      # Fit the scaler on the DataFrame and transform the data\n",
    "      normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "    else :\n",
    "      if fit:\n",
    "        normalized_data = scaler.fit_transform(df)\n",
    "      else:\n",
    "        normalized_data = scaler.transform(df)\n",
    "\n",
    "    # Create a new DataFrame with the scaled data\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "    # retourner le scaler\n",
    "    if return_scaler :\n",
    "      return normalized_df,scaler\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is made up of many features.\n",
    "\n",
    "\n",
    "**Creating Features**:\n",
    "\n",
    "As for the `date_forecast` feature. We saw during the signal processing analysis that this feature contains events that repeat at given periods (years/days), so we're going to split our feature into several sub-features to try and obtain relationships with the target.\n",
    "\n",
    "We therefore separate according to: *hours, day of the week, season, month, year, day of the year and day of the month*.\n",
    "\n",
    "In order to take into consideration the difference between *observed* and *estimated*, we can add a feature which represent the delay between the prediction and the reality of the weather. We will compute it by calculate the difference between `date_calc` and `date_forecast`.\n",
    "\n",
    "**Modification of Features**:\n",
    "\n",
    "When we explore our data we can see that `sun_azimuth:d` corresponds to the position of the sun in degrees. So we can see that 0 and 360 correspond to the same thing. So we're going to transform this feature using a cosine to make it continuous and consistent with what it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = X_train_observed_a['sun_azimuth:d'][0:199]\n",
    "transfo = extraction.apply(lambda x : np.cos(x/180*np.pi + np.pi))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(X_train_observed_a['date_forecast'][0:199],extraction/180-1,label=\"old\",ls=\"--\")\n",
    "plt.plot(X_train_observed_a['date_forecast'][0:199],transfo,label=\"new\",ls=\"--\")\n",
    "plt.plot(train_a['time'][0:49],train_a[\"pv_measurement\"][0:49]/2500-1,label=\"target\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"sun_azimuth modification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, label):\n",
    "    # creating features from dateforecast\n",
    "    df['hour'] = df[\"date_forecast\"].dt.hour\n",
    "    df['dayofweek'] = df[\"date_forecast\"].dt.dayofweek\n",
    "    df['quarter'] = df[\"date_forecast\"].dt.quarter\n",
    "    df['month'] = df[\"date_forecast\"].dt.month\n",
    "    df['year'] = df[\"date_forecast\"].dt.year\n",
    "    df['dayofyear'] = df[\"date_forecast\"].dt.dayofyear\n",
    "    df['dayofmonth'] = df[\"date_forecast\"].dt.day\n",
    "\n",
    "    # creating features from datecalc\n",
    "    if \"date_calc\" in df.columns :\n",
    "      \n",
    "      df['date_calc'] = pd.to_datetime(df['date_calc'])\n",
    "      df['date_calc'].fillna(df['date_forecast'], inplace=True)\n",
    "      df['delta_hour'] = df['hour'] - df[\"date_calc\"].dt.hour\n",
    "\n",
    "      df['correction'] = df['date_forecast'].dt.dayofyear - df['date_calc'].dt.dayofyear\n",
    "      df.loc[df['correction'] == 1, 'delta_hour'] += 24\n",
    "      df.drop('correction', axis=1, inplace=True)\n",
    "\n",
    "      df = df.drop([\"date_calc\",\"date_forecast\"],axis=1) \n",
    "    else :\n",
    "\n",
    "      df.drop([\"date_forecast\"],axis=1) \n",
    "      df['delta_hour'] = 0\n",
    "\n",
    "    # Modification of sun_azimuth\n",
    "    df['sun_azimuth:d'] = df['sun_azimuth:d'].apply(lambda x : np.cos(x/180*np.pi + np.pi))\n",
    "\n",
    "    # return the target if necessary\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        df = df.drop(label,axis=1)\n",
    "        return df, y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Management of constant parts of pv_measurement on B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely at the shape of pv_measurement on B and C, we can see that there are certain sections during which the feature doesn't vary even at night when it should.\n",
    "\n",
    "We therefore assume that these are erroneous measurements that need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates_when_constants(df, date_c = 'time', y = 'pv_measurement', delta = { 'days': 3 }):\n",
    "    df = df.copy()\n",
    "    mask_y_change = df[y] != df[y].shift(1)\n",
    "\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    constant_periods = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if not mask_y_change[index]:\n",
    "            if start_date is None:\n",
    "                start_date = row[date_c]\n",
    "            end_date = row[date_c]\n",
    "        else:\n",
    "            if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "                constant_periods.append((start_date, end_date))\n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "    if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "        constant_periods.append((start_date, end_date))\n",
    "    return constant_periods\n",
    "\n",
    "def delete_date_range_from_df(df, dates, date_c = 'time'):\n",
    "    df = df.copy()\n",
    "    c = 0\n",
    "    for start_date, end_date in dates:\n",
    "        mask = (df[date_c] >= start_date) & (df[date_c] < end_date)\n",
    "        df = df[~mask]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum duration\n",
    "delta = { 'hours': 12 * 4}\n",
    "\n",
    "# list of these\n",
    "filter_dates_when_constants(train_b, delta=delta)\n",
    "\n",
    "# Deletion B\n",
    "train_b_delete = delete_date_range_from_df(train_b, filter_dates_when_constants(train_b, delta=delta))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(train_b_delete[\"time\"],train_b_delete[\"pv_measurement\"],s=0.2)\n",
    "plt.grid()\n",
    "plt.title(\"B without constant periods\")\n",
    "plt.show()\n",
    "\n",
    "# Deletion C\n",
    "train_c_delete = delete_date_range_from_df(train_c, filter_dates_when_constants(train_c, delta=delta))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(train_c_delete[\"time\"],train_c_delete[\"pv_measurement\"],s=0.2)\n",
    "plt.grid()\n",
    "plt.title(\"C without constant periods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Interpolation of the output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our idea, which worked pretty well, was to interpolate the values of the output. We tryed different interpolations, but the one which looked as the best one to do so, were the linear one. It's mathically defined as following, and the function which do the job, is `interpolate_output_values`.$$y(t + 1/4) = 0.75 * y(t) + 0.25 * y(t+1)$$\n",
    "$$y(t + 1/2) = 0.5 * y(t) + 0.5 * y(t+1)$$\n",
    "$$y(t + 3/4) = 0.25 * y(t) + 0.75 * y(t+1)$$\n",
    "where t is a round hour (such as 12:00:00, 13:00:00 etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "interpolation_method = 'linear'\n",
    "def interpolate_output_values(df):\n",
    "    freq = '15T'\n",
    "    df = df.set_index('time').resample(freq).asfreq()\n",
    "    df['pv_measurement'] = df['pv_measurement'].interpolate(method=interpolation_method)\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "train_a_interpolated = interpolate_output_values(train_a)\n",
    "train_b_interpolated = interpolate_output_values(train_b)\n",
    "train_c_interpolated = interpolate_output_values(train_c)\n",
    "\n",
    "basic_range = train_a['time'].loc[3000:3023].index\n",
    "interpolation_range = train_a['time'].loc[4 * 3000: 4 * 3023].index\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(train_a['time'][basic_range], train_a['pv_measurement'][basic_range], label='pv_measurement values on A')\n",
    "plt.scatter(train_a_interpolated['time'][interpolation_range], train_a_interpolated['pv_measurement'][interpolation_range], color='orange', label='quarter-hourly linear-interpolation')\n",
    "plt.scatter(train_a['time'][basic_range], train_a['pv_measurement'][basic_range], label='pv_measurement values on A')\n",
    "plt.title('pv_measurement on A values and linear interpolation values of its self')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation permit us to create 4 times more data, on every location, which are, in a restrective way, similar to the original curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Input reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also thought about some ways to see from a different point of view the data. One was to create more inputs from the data of the past hour. Firstly, we had to select which one of the 4 lines is the most important one to predict the output. We suggested that the solar panel were quite reactive, which means that, for every hours, we have to take into consideration the value of the current hour. It means, for example, at anyday, for an output at the 12:00:00, we have to consider the values from X at 12:00:00. And, because we want to consider what happened during the last hour, we had, as new columns, the values at 11:15:00, 11:30:00 and 11:45:00. The code to create a such matrix is defined as `reshape_frame_to_match_output` (our advise is to not run this function because it require a lot of time to reshape the inputs, we recognize that its not much optimized). \n",
    "\n",
    "We had to take some others assumptions into consideration as well. For example, we have in the estimated-testing sets, the time line is not continuous. For example, some days are skipped, so we designed it such as, it duplicate the value of 23:45:00 every days (in order to not get nan values), and the input at 00:00:00 are taken 4 times every days. We considered that, that point was not much important to duplicate the data, because the sun's data are nill (there is no sun during the night as far we experimented it yet), so it should not influence much the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def reshape_frame_to_match_output(frame_input):\n",
    "    frame = frame_input[1:]\n",
    "    groups = [frame[i:i+4] for i in range(0, len(frame), 4)]\n",
    "\n",
    "    aggregated_groups = []\n",
    "    \n",
    "    first_input = pd.DataFrame(frame_input.loc[0])\n",
    "\n",
    "    group = pd.concat([first_input, first_input, first_input, first_input])\n",
    "    group_without_nan = group.fillna('')\n",
    "    new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "    aggregated_groups.append(new_input)     \n",
    "    \n",
    "    for group in tqdm(groups):\n",
    "        if (len(np.array(group['date_forecast'])) > 3):\n",
    "            if ((np.array(group['date_forecast']))[3].astype(datetime).time() == datetime(year=1970, month=1, day=1, hour=0, minute=0, second=0).time()):\n",
    "                group.reset_index(drop=True, inplace=True)\n",
    "                temp_group = group.loc[3]\n",
    "                group.loc[3] = group.loc[2]\n",
    "\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                aggregated_groups.append(new_input)\n",
    "\n",
    "                group.loc[0] = temp_group\n",
    "                group.loc[1] = temp_group\n",
    "                group.loc[2] = temp_group\n",
    "                group.loc[3] = temp_group\n",
    "\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                aggregated_groups.append(new_input)\n",
    "            else:\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                # if group == groups[0]: print(len(new_input))\n",
    "                aggregated_groups.append(new_input)\n",
    "        else:\n",
    "            group.reset_index(drop=True, inplace=True)\n",
    "            for k in range(4 - len(group)):\n",
    "                group = pd.concat([group, pd.DataFrame(group.loc[len(group) - 1]).T], ignore_index=True)\n",
    "            group_without_nan = group.fillna('')\n",
    "            new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "            aggregated_groups.append(new_input)\n",
    "            \n",
    "    new_table = pd.concat(aggregated_groups, axis=1, ignore_index=False).T\n",
    "\n",
    "    columns = []\n",
    "    for k in range(4):\n",
    "        [columns.append(f\"{c}_{k}\") for c in frame.keys()]\n",
    "\n",
    "    new_table.columns = columns\n",
    "    new_table.reset_index(drop=True, inplace=True)\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is called and run the following way (in comment because it takes to much time to run). We also added a function to save this dataframes to not have to run it everytimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_estimated_a_reshaped = reshape_frame_to_match_output(X_train_estimated_a)\n",
    "# X_train_observed_a_reshaped = reshape_frame_to_match_output(X_train_observed_a)\n",
    "# X_test_estimated_a_reshaped = reshape_frame_to_match_output(X_test_estimated_a)\n",
    "\n",
    "# X_train_estimated_b_reshaped = reshape_frame_to_match_output(X_train_estimated_b)\n",
    "# X_train_observed_b_reshaped = reshape_frame_to_match_output(X_train_observed_b)\n",
    "# X_test_estimated_b_reshaped = reshape_frame_to_match_output(X_test_estimated_b)\n",
    "\n",
    "# X_train_estimated_c_reshaped = reshape_frame_to_match_output(X_train_estimated_c)\n",
    "# X_train_observed_c_reshaped = reshape_frame_to_match_output(X_train_observed_c)\n",
    "# X_test_estimated_c_reshaped = reshape_frame_to_match_output(X_test_estimated_c)\n",
    "\n",
    "# files_names = ['X_train_estimated.parquet', 'X_train_observed.parquet', 'X_test_estimated.parquet']\n",
    "# folders = [ f\"{loc}_reshaped/\" for loc in locations ]\n",
    "# X_reshaped = [ X_train_estimated_a_reshaped, X_train_observed_a_reshaped, X_test_estimated_a_reshaped, X_train_estimated_b_reshaped, X_train_observed_b_reshaped, X_test_estimated_b_reshaped, X_train_estimated_c_reshaped, X_train_observed_c_reshaped, X_test_estimated_c_reshaped]\n",
    "\n",
    "# for folder in folders:\n",
    "#     for file in files_names:\n",
    "#         index_in_X = (files_names.index(file) + 1) * (folders.index(folder) + 1) - 1\n",
    "#         X_reshaped[index_in_X].replace('', np.nan).to_parquet(folder + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_preprocess(A = [X_train_observed_a, X_train_estimated_a], B = [X_train_observed_b, X_train_estimated_b], C = [X_train_observed_c, X_train_estimated_c], drop_columns = True):\n",
    "    X_total_a = pd.concat(A, ignore_index=True)\n",
    "    X_total_b = pd.concat(B, ignore_index=True)\n",
    "    X_total_c = pd.concat(C, ignore_index=True)\n",
    "\n",
    "    X_total_a_y = pd.merge(X_total_a, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    X_total_b_y = pd.merge(X_total_b, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    X_total_c_y = pd.merge(X_total_c, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    \n",
    "    X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "    X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "    X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "    if drop_columns:\n",
    "        X_total_a_y_nan = gestion_constant_columns(X_total_a_y_nan, 'A')\n",
    "        X_total_b_y_nan = gestion_constant_columns(X_total_b_y_nan, 'B')\n",
    "        X_total_c_y_nan = gestion_constant_columns(X_total_c_y_nan, 'C')\n",
    "    return X_total_a_y_nan, X_total_b_y_nan, X_total_c_y_nan\n",
    "\n",
    "X_total_a_y_nan, X_total_b_y_nan, X_total_c_y_nan = partial_preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Concacenate all locations in one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other pre-processing idea that we tryied were to make one model instead of doing three (one for each location). The following code do this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df_from_differents_locations(dfs):\n",
    "    dfs_to_concat = { k: df.copy() for (k, df) in dfs.items() }\n",
    "    for key, df in dfs_to_concat.items():\n",
    "        for loc in locations:\n",
    "            df[f'location_{loc}'] = 0\n",
    "        df[f'location_{key}'] = 1\n",
    "    dfs_to_concat_listed = [ df for _, df in dfs_to_concat.items() ]\n",
    "    df_to_return = pd.concat(dfs_to_concat_listed, ignore_index=True)\n",
    "    return df_to_return\n",
    "\n",
    "def drop_columns_for_concatenated_input(df):\n",
    "    columns_to_drop = []\n",
    "    columns_to_drop_a = X_test_estimated_b.std()[X_test_estimated_b.std() == 0].keys()\n",
    "    columns_to_drop_b = X_test_estimated_b.std()[X_test_estimated_b.std() == 0].keys()\n",
    "    columns_to_drop_c = X_test_estimated_b.std()[X_test_estimated_b.std() == 0].keys()\n",
    "    for column in columns_to_drop_a: \n",
    "        if column not in columns_to_drop: columns_to_drop.append(column)\n",
    "    for column in columns_to_drop_b:\n",
    "        if column not in columns_to_drop: columns_to_drop.append(column)\n",
    "    for column in columns_to_drop_c:\n",
    "        if column not in columns_to_drop: columns_to_drop.append(column)\n",
    "    return df.drop(columns=columns_to_drop)\n",
    "\n",
    "# THESE FUNCTIONS WILL BE EXPLAINED IN A FURTHER PART\n",
    "start_2019 = pd.to_datetime(\"2019-04-21\")\n",
    "end_2019 = pd.to_datetime(\"2019-07-22\")\n",
    "\n",
    "start_2020 = pd.to_datetime(\"2020-04-21\")\n",
    "end_2020 = pd.to_datetime(\"2020-07-22\")\n",
    "\n",
    "start_2021 = pd.to_datetime(\"2021-04-21\")\n",
    "end_2021 = pd.to_datetime(\"2021-07-22\")\n",
    "\n",
    "start_2022 = pd.to_datetime(\"2022-04-21\")\n",
    "end_2022 = pd.to_datetime(\"2022-07-22\")\n",
    "\n",
    "def create_mask_for_split_training_and_testing(df, start_estimated, time_column = 'date_forecast'):\n",
    "    mask_2020 = ((df[time_column] >= start_2020) & (df[time_column] < end_2020))\n",
    "    mask_2021 = ((df[time_column] >= start_2021) & (df[time_column] < end_2021))\n",
    "    mask_2022 = ((df[time_column] >= start_2022) & (df[time_column] < end_2022))\n",
    "    mask_estimated = (df[time_column] >= start_estimated)\n",
    "    return mask_2020, mask_2021, mask_2022,mask_estimated\n",
    "\n",
    "def split_training_testing_set(df, start_estimated, random_state=42, test_size = .1, time_column = 'date_forecast'):\n",
    "    df_to_split = df.copy()\n",
    "    mask_2020, mask_2021, mask_2022, mask_estimated = create_mask_for_split_training_and_testing(df_to_split, start_estimated)\n",
    "    df_summers = df_to_split[ mask_2020 | mask_2021 | mask_2022 | mask_estimated]\n",
    "    df_not_summer = df_to_split[~(mask_2020 | mask_2021 | mask_2022 | mask_estimated)]\n",
    "    test_size = test_size * (len(df_summers) + len(df_not_summer)) / len(df_summers)\n",
    "    train_data_summer, pv_test_not_ordered = train_test_split(df_summers, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    pv_train_not_ordered = pd.concat([train_data_summer, df_not_summer])\n",
    "    pv_train = pv_train_not_ordered.sort_values(by='date_forecast')\n",
    "    pv_test = pv_test_not_ordered.sort_values(by='date_forecast')\n",
    "    return pv_train, pv_test\n",
    "\n",
    "def concat_to_location_and_pre_process_for_training(\n",
    "        X_trains = { 'A': [X_train_observed_a, X_train_estimated_a], 'B': [X_train_observed_b, X_train_estimated_b], 'C': [X_train_observed_c, X_train_estimated_c]}, \n",
    "        X_tests =  { 'A': X_test_estimated_a, 'B': X_test_estimated_b, 'C': X_test_estimated_c}, \n",
    "        y_trains = { 'A': train_a, 'B': train_b, 'C': train_c}, \n",
    "        y_scalers = { 'A': StandardScaler(), 'B': StandardScaler(), 'C': StandardScaler()}\n",
    "):      \n",
    "        test_size, random_state = 0.1, 42\n",
    "        \n",
    "        X_total_a_y_nan, X_total_b_y_nan, X_total_c_y_nan = partial_preprocess(drop_columns=False)\n",
    "\n",
    "        X_total_a_y_nan = drop_columns_for_concatenated_input(X_total_a_y_nan)\n",
    "        X_total_b_y_nan = drop_columns_for_concatenated_input(X_total_b_y_nan)\n",
    "        X_total_c_y_nan = drop_columns_for_concatenated_input(X_total_c_y_nan)\n",
    "\n",
    "        pv_train_a, pv_test_a = split_training_testing_set(X_total_a_y_nan, X_train_estimated_a[\"date_forecast\"].mean(), test_size=test_size, random_state=random_state)\n",
    "        pv_train_b, pv_test_b = split_training_testing_set(X_total_b_y_nan, X_train_estimated_b[\"date_forecast\"].mean(), test_size=test_size, random_state=random_state)\n",
    "        pv_train_c, pv_test_c = split_training_testing_set(X_total_c_y_nan, X_train_estimated_c[\"date_forecast\"].mean(), test_size=test_size, random_state=random_state)\n",
    "\n",
    "        X_train_a, y_train_a = create_features(pv_train_a, label='pv_measurement')\n",
    "        X_test_a, y_test_a = create_features(pv_test_a, label='pv_measurement')\n",
    "\n",
    "        X_train_b, y_train_b = create_features(pv_train_b, label='pv_measurement')\n",
    "        X_test_b, y_test_b = create_features(pv_test_b, label='pv_measurement')\n",
    "\n",
    "        X_train_c, y_train_c = create_features(pv_train_c, label='pv_measurement')\n",
    "        X_test_c, y_test_c = create_features(pv_test_c, label='pv_measurement')\n",
    "\n",
    "        X_train_a_norm, y_scaler_a = sklearn_z_score_normalize_dataframe(X_train_a,return_scaler=True, scaler=y_scalers['A'])\n",
    "        X_train_b_norm, y_scaler_b = sklearn_z_score_normalize_dataframe(X_train_b,return_scaler=True, scaler=y_scalers['B'])\n",
    "        X_train_c_norm, y_scaler_c = sklearn_z_score_normalize_dataframe(X_train_c,return_scaler=True, scaler=y_scalers['C'])\n",
    "\n",
    "        X_test_a_norm = sklearn_z_score_normalize_dataframe(X_test_a,return_scaler=False,scaler=scaler_a)\n",
    "        X_test_b_norm = sklearn_z_score_normalize_dataframe(X_test_b,return_scaler=False,scaler=scaler_b)\n",
    "        X_test_c_norm = sklearn_z_score_normalize_dataframe(X_test_c,return_scaler=False,scaler=scaler_c)\n",
    "\n",
    "        X_train_to_concat = { 'A': X_train_a_norm, 'B': X_train_b_norm, 'C': X_train_c_norm }\n",
    "        X_train_norm = concat_df_from_differents_locations(X_train_to_concat)\n",
    "\n",
    "        X_test_to_concat = { 'A': X_test_a_norm, 'B': X_test_b_norm, 'C': X_test_c_norm }\n",
    "        X_test_norm = concat_df_from_differents_locations(X_test_to_concat)\n",
    "\n",
    "        y_train_a, y_scaler_a = sklearn_z_score_normalize_dataframe(pd.DataFrame(y_train_a),return_scaler=True,scaler=y_scaler_a)\n",
    "        y_train_b, y_scaler_b = sklearn_z_score_normalize_dataframe(pd.DataFrame(y_train_b),return_scaler=True,scaler=y_scaler_b)\n",
    "        y_train_c, y_scaler_c = sklearn_z_score_normalize_dataframe(pd.DataFrame(y_train_c),return_scaler=True,scaler=y_scaler_c)\n",
    "\n",
    "        y_test_a = pd.DataFrame(y_scaler_a.transform(pd.DataFrame(y_test_a)))\n",
    "        y_test_b = pd.DataFrame(y_scaler_b.transform(pd.DataFrame(y_test_b)))\n",
    "        y_test_c = pd.DataFrame(y_scaler_c.transform(pd.DataFrame(y_test_c)))\n",
    "\n",
    "        y_train = pd.concat([y_train_a, y_train_b, y_train_c], ignore_index=True)\n",
    "        y_test= pd.concat([y_test_a, y_test_b, y_test_c], ignore_index=True)\n",
    "        \n",
    "        y_scalers = { 'A': y_scaler_a, 'B': y_scaler_b, 'C': y_scaler_c}\n",
    "\n",
    "        return X_train_norm, X_test_norm, y_train, y_test, y_scalers\n",
    "\n",
    "def postprocess_concatenated_input(preds, y_scalers):\n",
    "    pred_a = y_scalers['A'].inverse_transform(pd.DataFrame({'pv_measurement_prediction': preds['A']})).reshape(-1)\n",
    "    pred_b = y_scalers['B'].inverse_transform(pd.DataFrame({'pv_measurement_prediction': preds['B']})).reshape(-1)\n",
    "    pred_c = y_scalers['C'].inverse_transform(pd.DataFrame({'pv_measurement_prediction': preds['C']})).reshape(-1)\n",
    "    return np.concatenate((np.concatenate((pred_a,pred_b)), pred_c))\n",
    "     \n",
    "X_train_norm, X_test_norm, y_train, y_test, y_scalers = concat_to_location_and_pre_process_for_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-processing ideas that didn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find new relationships between features, we're going to apply a method called `Polynomial Features`, which consists of adding new features to the dataset that are the products of those already present.\n",
    "\n",
    "In our dataset, we currently have over 50 features. We can't take all the combinations, as too many features would prevent the model from finding good combinations (with very long training times). So we're going to run a first model without `Polynomial Features` to get a ranking of the importance of the features, so as to take only a certain number. This is what the `recup_var` function does.\n",
    "\n",
    "We then add to our dataframe the polynomial combinations of these features to the desired degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recup_var(reg,number=10):\n",
    "  feature_importance = reg.get_booster().get_fscore()\n",
    "  sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_features = [feature for feature, _ in sorted_feature_importance[:number]]\n",
    "  return top_features\n",
    "\n",
    "def column_filter(df,columns):\n",
    "  df_filtre = df[columns]\n",
    "  return df_filtre\n",
    "\n",
    "def creation_df_polynomial_feature(df,degre=1):\n",
    "\n",
    "  # Creation of a PolynomialFeatures object\n",
    "  poly = PolynomialFeatures(degre, include_bias=False)\n",
    "\n",
    "  # Apply the polynomial caractéristiques to the datas\n",
    "  poly_features = poly.fit_transform(df)\n",
    "\n",
    "  # Create a new dataframe\n",
    "  poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(input_features=df.columns))\n",
    "\n",
    "  return poly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the results obtained by testing several combinations of features did not improve the score. We therefore decided to shelve this avenue.\n",
    "\n",
    "If you'd like to implement it and test it with our code, all you have to do is train a model and put it into the recup_var function, then retrieve the filtered dataframe with the filter_column function. Finally, just put it into the creation_df_polynomial_feature function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mean of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that dataframes containing features are sliced by quarter-hour, whereas those containing targets are sliced by hour.\n",
    "\n",
    "To try and take into account what happens in the hour as weather (as it impacts `pv_measurement`), we tried to take the average of the different weather components in the hour.\n",
    "\n",
    "Please note that in the test file, the days are taken at random and we start our predictions at 0h, so we need to take this into account. This is why our code will follow the following algorithm every 24h:\n",
    "* take the weather at 0h\n",
    "* remove the weather at 0h and 23h15/30/45\n",
    "* add the average weather for the 4 lines every 4 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traitement_df(df_input):\n",
    "\n",
    "    if \"date_calc\" in list(df_input.columns):\n",
    "      df_input = df_input.drop([\"date_calc\"],axis=1)\n",
    "\n",
    "    df_output = pd.DataFrame(columns=df_input.columns)\n",
    "\n",
    "    aux = df_input[df_input.index % 4 == 0]\n",
    "    date_col = aux[\"date_forecast\"]\n",
    "    date_col.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_input = df_input.drop([\"date_forecast\"],axis=1)\n",
    "\n",
    "    for i in range(0, len(df_input), 96):\n",
    "        df_etude = df_input.iloc[i:i+96]\n",
    "\n",
    "        first_row = df_etude.iloc[0]\n",
    "        df_output = pd.concat([df_output, first_row.to_frame().transpose()], ignore_index=True)\n",
    "\n",
    "        df_etude = df_etude.iloc[1:-3]\n",
    "        df_etude.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        means = df_etude.groupby(df_etude.index // 4).mean()\n",
    "        df_output = pd.concat([df_output, means], ignore_index=True)\n",
    "\n",
    "    df_output[\"date_forecast\"] = date_col\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Separation of training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our study, we used different ways to split the training and test sets. We found out that some ways were more efficient than some others. We first used a \"classical approach\", which consisted by taking last 10% of the whole training set (observed and estimated) for each locations. We then realized that, because our predictions would be during a date range of spring/summer, we must had to include some dates from this time in our test set. Moreover, our predictions will be based on estimated values, which means that we have to take that kind of values into consideration, in order to get the distribution of estimated values in our prevision as well.\n",
    "\n",
    "So we are going to present the first split we did, which were less efficient, than the second one that we are going to present then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Split training and test sets on estimated set dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_testing_sets_on_test_set_dates():\n",
    "    quantile = .25\n",
    "    pv_train_a = X_total_a_y_nan[X_total_a_y_nan['date_forecast'] <= X_train_estimated_a['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_a = X_total_a_y_nan[X_total_a_y_nan['date_forecast'] > X_train_estimated_a['date_forecast'].quantile(quantile)].copy()\n",
    "\n",
    "    pv_train_b = X_total_b_y_nan[X_total_b_y_nan['date_forecast'] <= X_train_estimated_b['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_b = X_total_b_y_nan[X_total_b_y_nan['date_forecast'] > X_train_estimated_b['date_forecast'].quantile(quantile)].copy()\n",
    "\n",
    "    pv_train_c = X_total_c_y_nan[X_total_c_y_nan['date_forecast'] <= X_train_estimated_c['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_c = X_total_c_y_nan[X_total_c_y_nan['date_forecast'] > X_train_estimated_c['date_forecast'].quantile(quantile)].copy() \n",
    "    return pv_train_a, pv_test_a, pv_train_b, pv_test_b, pv_train_c, pv_test_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Split training and test sets on prediction dates range and estimated set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_2020 = pd.to_datetime(\"2020-04-21\")\n",
    "end_2020 = pd.to_datetime(\"2020-07-22\")\n",
    "\n",
    "start_2021 = pd.to_datetime(\"2021-04-21\")\n",
    "end_2021 = pd.to_datetime(\"2021-07-22\")\n",
    "\n",
    "start_2022 = pd.to_datetime(\"2022-04-21\")\n",
    "end_2022 = pd.to_datetime(\"2022-07-22\")\n",
    "\n",
    "def create_mask_for_split_training_and_testing(df, start_estimated, time_column = 'date_forecast'):\n",
    "    mask_2020 = ((df[time_column] >= start_2020) & (df[time_column] < end_2020))\n",
    "    mask_2021 = ((df[time_column] >= start_2021) & (df[time_column] < end_2021))\n",
    "    mask_2022 = ((df[time_column] >= start_2022) & (df[time_column] < end_2022))\n",
    "    mask_estimated = (df[time_column] >= start_estimated)\n",
    "    return mask_2020, mask_2021, mask_2022,mask_estimated\n",
    "\n",
    "def split_training_testing_set(df, start_estimated, random_state=42, test_size = .1, time_column = 'date_forecast'):\n",
    "    df_to_split = df.copy()\n",
    "    mask_2020, mask_2021, mask_2022, mask_estimated = create_mask_for_split_training_and_testing(df_to_split, start_estimated)\n",
    "    df_summers = df_to_split[ mask_2020 | mask_2021 | mask_2022 | mask_estimated]\n",
    "    df_not_summer = df_to_split[~(mask_2020 | mask_2021 | mask_2022 | mask_estimated)]\n",
    "    test_size = test_size * (len(df_summers) + len(df_not_summer)) / len(df_summers)\n",
    "    train_data_summer, pv_test_not_ordered = train_test_split(df_summers, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    pv_train_not_ordered = pd.concat([train_data_summer, df_not_summer])\n",
    "    pv_train = pv_train_not_ordered.sort_values(by='date_forecast')\n",
    "    pv_test = pv_test_not_ordered.sort_values(by='date_forecast')\n",
    "    return pv_train, pv_test\n",
    "\n",
    "random_state = 42\n",
    "pv_train_a, pv_test_a = split_training_testing_set(X_total_a_y_nan, X_train_estimated_a[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_b, pv_test_b = split_training_testing_set(X_total_b_y_nan, X_train_estimated_b[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_c, pv_test_c = split_training_testing_set(X_total_c_y_nan, X_train_estimated_c[\"date_forecast\"].mean(), random_state=random_state)\n",
    "\n",
    "print(\"train_a :\",pv_train_a.shape)\n",
    "print(\"test_a :\",pv_test_a.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_a.shape[0]/(pv_test_a.shape[0]+pv_train_a.shape[0]),3)*100, '%')\n",
    "print(\"train_b :\",pv_train_b.shape)\n",
    "print(\"test_b :\",pv_test_b.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_b.shape[0]/(pv_test_b.shape[0]+pv_train_b.shape[0]),3)*100, '%')\n",
    "print(\"train_c :\",pv_train_c.shape)\n",
    "print(\"test_c :\",pv_test_c.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_c.shape[0]/(pv_test_c.shape[0]+pv_train_c.shape[0]),3)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Models performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the reading of our study, we will only detail our results for location A.\n",
    "\n",
    "What's more, A has the highest target values (4 to 5 times higher than B and C), which gives us a good idea of the trend to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datas\n",
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "# observed + estimated\n",
    "X_total_a = pd.concat([X_train_observed_a,X_train_estimated_a])\n",
    "X_total_b = pd.concat([X_train_observed_b,X_train_estimated_b])\n",
    "X_total_c = pd.concat([X_train_observed_c,X_train_estimated_c])\n",
    "\n",
    "# Rename + Delete lignes without pv_measurement\n",
    "train_a.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_b.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_c.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "train_a.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_b.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_c.dropna(subset=['pv_measurement'], inplace=True)\n",
    "\n",
    "# Match between features and target\n",
    "X_total_a_y = pd.merge(X_total_a, train_a, on='date_forecast', how='inner')\n",
    "X_total_b_y = pd.merge(X_total_b, train_b, on='date_forecast', how='inner')\n",
    "X_total_c_y = pd.merge(X_total_c, train_c, on='date_forecast', how='inner')\n",
    "\n",
    "# Nan\n",
    "X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "# Train/Test split\n",
    "split_date_a = X_train_estimated_a['date_forecast'].quantile(0.25)\n",
    "split_date_b = X_train_estimated_b['date_forecast'].quantile(0.25)\n",
    "split_date_c = X_train_estimated_c['date_forecast'].quantile(0.25)\n",
    "\n",
    "pv_train_a, pv_test_a, pv_train_b, pv_test_b, pv_train_c, pv_test_c = split_training_testing_sets_on_test_set_dates()\n",
    "\n",
    "# Date_forecast type\n",
    "pv_train_a['date_forecast'] = pd.to_datetime(pv_train_a['date_forecast'])\n",
    "pv_test_a['date_forecast'] = pd.to_datetime(pv_test_a['date_forecast'])\n",
    "\n",
    "pv_train_b['date_forecast'] = pd.to_datetime(pv_train_b['date_forecast'])\n",
    "pv_test_b['date_forecast'] = pd.to_datetime(pv_test_b['date_forecast'])\n",
    "\n",
    "pv_train_c['date_forecast'] = pd.to_datetime(pv_train_c['date_forecast'])\n",
    "pv_test_c['date_forecast'] = pd.to_datetime(pv_test_c['date_forecast'])\n",
    "\n",
    "\n",
    "#feature Engineering\n",
    "X_train_a, y_train_a = create_features(pv_train_a, label='pv_measurement')\n",
    "X_test_a, y_test_a = create_features(pv_test_a, label='pv_measurement')\n",
    "\n",
    "X_train_b, y_train_b = create_features(pv_train_b, label='pv_measurement')\n",
    "X_test_b, y_test_b = create_features(pv_test_b, label='pv_measurement')\n",
    "\n",
    "X_train_c, y_train_c = create_features(pv_train_c, label='pv_measurement')\n",
    "X_test_c, y_test_c = create_features(pv_test_c, label='pv_measurement')\n",
    "\n",
    "# Normalisation\n",
    "scaler_a = StandardScaler()\n",
    "scaler_b = StandardScaler()\n",
    "scaler_c = StandardScaler()\n",
    "\n",
    "X_train_a_norm,scaler_a = sklearn_z_score_normalize_dataframe(X_train_a,return_scaler=True, scaler=scaler_a)\n",
    "X_train_b_norm,scaler_b = sklearn_z_score_normalize_dataframe(X_train_b,return_scaler=True, scaler=scaler_b)\n",
    "X_train_c_norm,scaler_c = sklearn_z_score_normalize_dataframe(X_train_c,return_scaler=True, scaler=scaler_c)\n",
    "\n",
    "X_test_a_norm = sklearn_z_score_normalize_dataframe(X_test_a,return_scaler=False,scaler=scaler_a, fit=False)\n",
    "X_test_b_norm = sklearn_z_score_normalize_dataframe(X_test_b,return_scaler=False,scaler=scaler_b, fit=False)\n",
    "X_test_c_norm = sklearn_z_score_normalize_dataframe(X_test_c,return_scaler=False,scaler=scaler_c, fit=False)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42) #, max_depth = 10\n",
    "rf_model.fit(X_train_a_norm, y_train_a)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42) #learning_rate=0.1,\n",
    "gb_model.fit(X_train_a_norm, y_train_a)\n",
    "\n",
    "# Créez un modèle XGBoost\n",
    "reg_a = xgb.XGBRegressor(n_estimators=100)\n",
    "reg_a.fit(X_train_a_norm, y_train_a,\n",
    "          eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "# Liste pour stocker les erreurs de test à chaque itération\n",
    "valid_errors = []\n",
    "\n",
    "for i, y_pred in enumerate(gb_model.staged_predict(X_test_a_norm)):\n",
    "    valid_errors.append(mean_squared_error(y_test_a, y_pred))\n",
    "\n",
    "# Créez une figure pour chaque taux d'apprentissage\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Learning curve Gradient Boosting\")\n",
    "plt.plot(\n",
    "    np.arange(100) + 1,\n",
    "    gb_model.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(100) + 1, valid_errors, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# XGBOOST\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "results = reg_a.evals_result()\n",
    "train_errors = results['validation_0'][\"rmse\"]\n",
    "test_errors = results['validation_1'][\"rmse\"]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.legend()\n",
    "plt.title('Learning Curves XGBoost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "rf_predictions = rf_model.predict(X_test_a_norm)\n",
    "gb_predictions = gb_model.predict(X_test_a_norm)\n",
    "reg_a_predictions = reg_a.predict(X_test_a_norm)\n",
    "\n",
    "# Evaluation\n",
    "rf_mse = mean_squared_error(y_test_a, rf_predictions)\n",
    "gb_mse = mean_squared_error(y_test_a, gb_predictions)\n",
    "reg_a_mse = mean_squared_error(y_test_a, reg_a_predictions)\n",
    "print(f\"Random Forest MSE: {round(rf_mse,0)}\")\n",
    "print(f\"Gradient Boosting MSE: {round(gb_mse,0)}\")\n",
    "print(f\"XGBoost MSE: {round(reg_a_mse,0)}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "mae_rf = np.mean(np.abs(y_test_a - rf_predictions))\n",
    "mae_gb = np.mean(np.abs(y_test_a - gb_predictions))\n",
    "mae_reg_a = np.mean(np.abs(y_test_a - reg_a_predictions))\n",
    "print(f\"Random Forest MAE: {round(mae_rf,2)}\")\n",
    "print(f\"Gradient Boosting MAE: {round(mae_gb,2)}\")\n",
    "print(f\"XGBoost MAE: {round(mae_reg_a,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these first results, we can see that the models have converged well (see learning curves above). We first chose to train our models using RMSE, as it allows us to take better account of outliers (if we look at the distribution of pv_measurement we see that values can reach large values, but quite rarely).\n",
    "\n",
    "In the evaluation phase, we can see that on both measures, the XGBoost model performs much better than the other two models.\n",
    "\n",
    "In addition to this study, we can look at the relative importance of each variable on the model by plotting the importance graph generated by the \"tree\" type models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(reg_a, height=.9)\n",
    "plt.gcf().set_size_inches(14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this ranking, we can see a predominance of 3 variables: `absolute_humidity`,`diffuse_rad`,`direct_rad`.\n",
    "\n",
    "This seems coherent, as these variables have a direct impact on the sun's rays. As for the radiation variables, these relate directly to sunshine characteristics, which will be proportional to the power generated on the solar panels.\n",
    "\n",
    "As for humidity, we can deduce that water droplets in the air have a major impact on light transmission and reflection, and therefore on the deduced power generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. XGBoost hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus decided to focus on a XGBoost model, which implies creating models (for A, B and C) with the best hyper-parameters. We thus made the following code in order to find the best hyper-parameters. The thing is, that algorithm has an exponential complexity in terms of the number of hyper-parameters we want to try. \n",
    "Thus to make it converge, we were tooking the last best-values, and thus we tryed the neighboors values. And we reapeated it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datas\n",
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "# Interpolate values\n",
    "train_a = interpolate_output_values(train_a)\n",
    "train_b = interpolate_output_values(train_b)\n",
    "train_c = interpolate_output_values(train_c)\n",
    "\n",
    "# observed + estimated\n",
    "X_total_a = pd.concat([X_train_observed_a,X_train_estimated_a])\n",
    "X_total_b = pd.concat([X_train_observed_b,X_train_estimated_b])\n",
    "X_total_c = pd.concat([X_train_observed_c,X_train_estimated_c])\n",
    "\n",
    "# Rename + Delete lignes without pv_measurement\n",
    "train_a.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_b.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_c.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "train_a.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_b.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_c.dropna(subset=['pv_measurement'], inplace=True)\n",
    "# Match between features and target\n",
    "X_total_a_y = pd.merge(X_total_a, train_a, on='date_forecast', how='inner')\n",
    "X_total_b_y = pd.merge(X_total_b, train_b, on='date_forecast', how='inner')\n",
    "X_total_c_y = pd.merge(X_total_c, train_c, on='date_forecast', how='inner')\n",
    "\n",
    "# Nan\n",
    "X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "# Train/Test split\n",
    "pv_train_a, pv_test_a = split_training_testing_set(X_total_a_y_nan, X_train_estimated_a[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_b, pv_test_b = split_training_testing_set(X_total_b_y_nan, X_train_estimated_b[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_c, pv_test_c = split_training_testing_set(X_total_c_y_nan, X_train_estimated_c[\"date_forecast\"].mean(), random_state=random_state)\n",
    "\n",
    "# Date_forecast type\n",
    "pv_train_a['date_forecast'] = pd.to_datetime(pv_train_a['date_forecast'])\n",
    "pv_test_a['date_forecast'] = pd.to_datetime(pv_test_a['date_forecast'])\n",
    "\n",
    "pv_train_b['date_forecast'] = pd.to_datetime(pv_train_b['date_forecast'])\n",
    "pv_test_b['date_forecast'] = pd.to_datetime(pv_test_b['date_forecast'])\n",
    "\n",
    "pv_train_c['date_forecast'] = pd.to_datetime(pv_train_c['date_forecast'])\n",
    "pv_test_c['date_forecast'] = pd.to_datetime(pv_test_c['date_forecast'])\n",
    "\n",
    "\n",
    "#feature Engineering\n",
    "X_train_a, y_train_a = create_features(pv_train_a, label='pv_measurement')\n",
    "X_test_a, y_test_a = create_features(pv_test_a, label='pv_measurement')\n",
    "\n",
    "X_train_b, y_train_b = create_features(pv_train_b, label='pv_measurement')\n",
    "X_test_b, y_test_b = create_features(pv_test_b, label='pv_measurement')\n",
    "\n",
    "X_train_c, y_train_c = create_features(pv_train_c, label='pv_measurement')\n",
    "X_test_c, y_test_c = create_features(pv_test_c, label='pv_measurement')\n",
    "\n",
    "X_train_a = gestion_constant_columns(X_train_a, 'A')\n",
    "X_test_a = gestion_constant_columns(X_test_a, 'A')\n",
    "\n",
    "X_train_b = gestion_constant_columns(X_train_b, 'B')\n",
    "X_test_b = gestion_constant_columns(X_test_b, 'B')\n",
    "\n",
    "X_train_c = gestion_constant_columns(X_train_c, 'C')\n",
    "X_test_c = gestion_constant_columns(X_test_c, 'C')\n",
    "\n",
    "# Normalisation\n",
    "scaler_a = StandardScaler()\n",
    "scaler_b = StandardScaler()\n",
    "scaler_c = StandardScaler()\n",
    "\n",
    "X_train_a_norm,scaler_a = sklearn_z_score_normalize_dataframe(X_train_a,return_scaler=True, scaler=scaler_a)\n",
    "X_train_b_norm,scaler_b = sklearn_z_score_normalize_dataframe(X_train_b,return_scaler=True, scaler=scaler_b)\n",
    "X_train_c_norm,scaler_c = sklearn_z_score_normalize_dataframe(X_train_c,return_scaler=True, scaler=scaler_c)\n",
    "\n",
    "X_test_a_norm = sklearn_z_score_normalize_dataframe(X_test_a,return_scaler=False,scaler=scaler_a, fit=False)\n",
    "X_test_b_norm = sklearn_z_score_normalize_dataframe(X_test_b,return_scaler=False,scaler=scaler_b, fit=False)\n",
    "X_test_c_norm = sklearn_z_score_normalize_dataframe(X_test_c,return_scaler=False,scaler=scaler_c, fit=False)\n",
    "\n",
    "# THE BEST HYPER-PARAMETERS WE FOUND OUT\n",
    "eval_metric = \"rmse\"\n",
    "hyper_params_a = {'subsample': 0.6, 'refresh_leaf': 1, 'n_estimators': 4000, 'min_child_weight': 15, 'max_depth': 4, 'gamma': 0.5, 'eta': 0.005, 'colsample_bytree': 0.8}\n",
    "hyper_params_b = {'n_estimators': 4000, 'subsample': 0.6, 'refresh_leaf': 1, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 0.5, 'eta': 0.01, 'colsample_bytree': 0.6}\n",
    "hyper_params_c = {'n_estimators': 4000, 'subsample': 0.6, 'refresh_leaf': 1, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_a = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_a\n",
    "    )\n",
    "\n",
    "reg_b = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_b\n",
    "    )\n",
    "\n",
    "reg_c = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_c\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of hyper-parameters range to find the best suit of it\n",
    "\n",
    "gen_params = {\n",
    "        'n_estimators': [2000, 3000, 3500],\n",
    "        'gamma': [0.5, 1, 5],\n",
    "        'subsample': [0.6, 1.0],\n",
    "        'max_depth': [4, 5],\n",
    "        'eta': [ 0.005, 0.01, 0.05],\n",
    "        'n_estimators': [ 3000, 4000 ]\n",
    "        }\n",
    "\n",
    "params_a = {\n",
    "        'min_child_weight': [ 10, 12, 15 ],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'refresh_leaf': [ .7, 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_b = {\n",
    "        'min_child_weight': [3, 5, 7],\n",
    "        'colsample_bytree': [.7, 0.6, .5],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_c = {\n",
    "        'min_child_weight': [3, 5, 7],\n",
    "        'colsample_bytree': [ 0.6 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "# An example of hyper-parameters range, based on the best one founds, \n",
    "# to run more quickly the program\n",
    "\n",
    "gen_params = {\n",
    "        'n_estimators': [5000],\n",
    "        'gamma': [0.5],\n",
    "        'subsample': [0.6],\n",
    "        'n_estimators': [5000]\n",
    "        }\n",
    "\n",
    "params_a = {\n",
    "        'min_child_weight': [ 15, 17 ],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'max_depth': [4],\n",
    "        'eta': [ 0.005 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_b = {\n",
    "        'min_child_weight': [5],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'max_depth': [5],\n",
    "        'eta': [ 0.01 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_c = {\n",
    "        'min_child_weight': [5],\n",
    "        'colsample_bytree': [1],\n",
    "        'max_depth': [5],\n",
    "        'eta': [ 0.05 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_comb_a = reduce(lambda x,y: x*y, [len(v) for v in params_a.values()])\n",
    "param_comb_b = reduce(lambda x,y: x*y, [len(v) for v in params_b.values()])\n",
    "param_comb_c = reduce(lambda x,y: x*y, [len(v) for v in params_c.values()])\n",
    "n_jobs = 10\n",
    "\n",
    "\n",
    "random_search_a = RandomizedSearchCV(reg_a, param_distributions=params_a, n_iter=param_comb_a, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )\n",
    "random_search_b = RandomizedSearchCV(reg_b, param_distributions=params_b, n_iter=param_comb_b, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )\n",
    "random_search_c = RandomizedSearchCV(reg_c, param_distributions=params_c, n_iter=param_comb_c, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )\n",
    "\n",
    "random_search_a.fit(\n",
    "    X_train_a_norm, y_train_a,\n",
    "    eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "print('\\n All results:')\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_a.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_a.best_params_)\n",
    "\n",
    "random_search_b.fit(\n",
    "    X_train_b_norm, y_train_b,\n",
    "    eval_set=[(X_train_b_norm, y_train_b), (X_test_b_norm, y_test_b)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search_b.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_b.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_b.best_params_)\n",
    "\n",
    "random_search_c.fit(\n",
    "    X_train_c_norm, y_train_c,\n",
    "    eval_set=[(X_train_c_norm, y_train_c), (X_test_c_norm, y_test_c)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search_c.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_c.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_c.best_params_)\n",
    "\n",
    "reg_a = random_search_a.best_estimator_\n",
    "reg_b = random_search_b.best_estimator_\n",
    "reg_c = random_search_c.best_estimator_\n",
    "\n",
    "reg_a.fit(X_train_a_norm, y_train_a,\n",
    "          eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "reg_b.fit(X_train_b_norm, y_train_b,\n",
    "          eval_set=[(X_train_b_norm, y_train_b), (X_test_b_norm, y_test_b)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "reg_c.fit(X_train_c_norm, y_train_c,\n",
    "          eval_set=[(X_train_c_norm, y_train_c), (X_test_c_norm, y_test_c)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "\n",
    "# Créez des listes vides pour stocker les erreurs d'entraînement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accédez aux erreurs d'entraînement et de test après chaque itération\n",
    "results = reg_a.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()\n",
    "\n",
    "min_error_a = min(test_errors)\n",
    "# Créez des listes vides pour stocker les erreurs d'entraînement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accédez aux erreurs d'entraînement et de test après chaque itération\n",
    "results = reg_b.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()\n",
    "\n",
    "min_error_b = min(test_errors)\n",
    "# Créez des listes vides pour stocker les erreurs d'entraînement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accédez aux erreurs d'entraînement et de test après chaque itération\n",
    "results = reg_c.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "min_error_c = min(test_errors)\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered during our study that, as we already said, that the results of this algorithm, to find to best parameters were strongly correlated. Our submissions on kaggle were depending a lot of which test sets we choose. The thing is it was very though, and required a lot of time, to look for the best test set and and find the model with the best hyperparameters on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(reg_a, height=.9)\n",
    "plt.gcf().set_size_inches(14, 8)\n",
    "plot_importance(reg_b, height=0.9)\n",
    "plt.gcf().set_size_inches(14, 8)\n",
    "plot_importance(reg_c, height=0.9)\n",
    "plt.gcf().set_size_inches(14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look on the results that we get from the last model, we see that we get some values lowest to zero. If we look then on the training set, the values are not null (which makes sense). So we decided to add a little function to replace all the values below 10 by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_test(df, scaler, loc):\n",
    "  X_test = df.drop([\"id\",\"location\",\"prediction\"],axis=1)\n",
    "  X_test = gestion_constant_columns(X_test, loc)\n",
    "  X_test = create_features(X_test, None)\n",
    "  X_test = gestion_nan(X_test)\n",
    "  X_test = sklearn_z_score_normalize_dataframe(X_test,return_scaler=False,scaler=scaler, fit=False)\n",
    "  return X_test\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_copy = test.copy()\n",
    "test.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "test[\"date_forecast\"] = pd.to_datetime(test[\"date_forecast\"])\n",
    "\n",
    "merged_df_a = pd.merge(X_test_estimated_a, test, on='date_forecast', how='inner')\n",
    "merged_df_b = pd.merge(X_test_estimated_b, test, on='date_forecast', how='inner')\n",
    "merged_df_c = pd.merge(X_test_estimated_c, test, on='date_forecast', how='inner')\n",
    "\n",
    "X_test_a_test = preprocessing_test(merged_df_a, scaler_a, 'A')\n",
    "X_test_b_test = preprocessing_test(merged_df_b, scaler_b, 'B')\n",
    "X_test_c_test = preprocessing_test(merged_df_c, scaler_c, 'C')\n",
    "\n",
    "result_A = reg_a.predict(X_test_a_test)\n",
    "result_B = reg_b.predict(X_test_b_test)\n",
    "result_C = reg_c.predict(X_test_c_test)\n",
    "\n",
    "results = np.concatenate((np.concatenate((result_A,result_B)), result_C))\n",
    "results[results <  10] = 0\n",
    "\n",
    "plt.plot(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Some other models which did not succeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced parameters to run quickly\n",
    "epochs = 500\n",
    "max_trial = 1\n",
    "batch_size = 256\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]\n",
    "# objective='val_root_mean_squared_error'\n",
    "\n",
    "# It tries 10 different models.\n",
    "reg_a = ak.StructuredDataRegressor(\n",
    "    max_trials=max_trial,\n",
    "    metrics=metrics,\n",
    "    overwrite=True,\n",
    "    project_name='regA1',\n",
    "    # objective=objective\n",
    "    )\n",
    "\n",
    "reg_a.fit(\n",
    "    X_train_a_norm, \n",
    "    y_train_a, \n",
    "    validation_data=(X_test_a_norm, y_test_a),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "\n",
    "predicted_y = reg_a.predict(X_test_a_norm)\n",
    "\n",
    "print(reg_a.evaluate(X_test_a_norm, y_test_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Areas for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to present the different areas of improvement we thought to do, but we did not had the time to implement them. However, we think that those areas would be interesting to think about, and could be interesting to develop, according to this project, or for a further one of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Management of the training and test set:\n",
    "    \n",
    "    A first thing, which we already mentionned, and was a big challenge to upgrade, was the split between the training and testing set. We tryed many split and some of our upgrades which were an improvement on one test set, were not on an another. So it was really hard to deal with this. However, this is probably a common problem in machine learning, but we were still strugling with this. An idea, to fit the most possible the data we want to predict, were to adjust are testing set on the days which has similar values that the one we want to estimate. On the other hand, we wanted that our model must be the most general as we could so we did not do it at the first time. \n",
    "\n",
    "- Management of the observed/estimated set:\n",
    "\n",
    "    A big issue that we faced with was how to take into consideration the difference of distribution between the observed and estimated set. The thing is, our biggest sets are the observed ones and on the other hand, we had to predict the values on estimated sets. Our only improvment in the submitted models, was to take some values in both training and testing sets. However, we could have improve that part by doing a lot of things but somes were requiring to much time to make it. For example, we thought about making kind of a GAN (generative adversarial network) to generate a convertissor from estimated values to observed ones, based on the `pv_measurement` values. However that kind of idea requires too much time to be very efficient so we did not focused much on it.\n",
    "\n",
    "- Location combination:\n",
    "\n",
    "    We experimented in our study to do a single model instead of three. It did not worked well but we figured out at the last moment that it could have been interesting to do 2, one for A and one for B and C because the hyperparameters of this two locations were very similar.\n",
    "\n",
    "- Model combination:\n",
    "\n",
    "    An idea that we had to improve the model, were to make some different models. We thought, as an intuitive way, that perhaps some models, with some hypermeters are better to predict on bad weather, and some, with other hyperparameters are better to predict on good weather. This is a lonely aspect, but we could take much more into consideration. Our idea would have be at first, with for example, two models which predicts a value each, $\\hat{y_1}$ and $\\hat{y_2}$, our final prediction would be a linear combination of those which makes our predicted value as: $$\\hat{y} = \\alpha * \\hat{y_1} + \\beta * \\hat{y_2}$$\n",
    "    Thus, this model could be also improved with a model which predict, based on the inputs, those coefficients $\\alpha$ and $\\beta$. The coefficients, would not be the same depending on the weather.\n",
    "\n",
    "    We can notice that, that idea is not restricted on two, but we can had a lot more.\n",
    "\n",
    "- Reduce the number of the features on our reshaped model:\n",
    "\n",
    "    An other we thought about, was to reduce the number of features on our reshaped model based on the importance of them. Thus, we would have different models, one after the other, reducing the number of features, little by little.\n",
    "\n",
    "- Trying other Machine Learning models:\n",
    "\n",
    "    There are a lot more Machine Learning models existing nowdays but we only mainly focused on XGBoost, because a lack of time from us, spending to much time at first to find more difficult ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are goning to summarize all we did and said concerning our study on this project, analyze and critic our methods, what worked well and failed for our futher projects and thus, make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we tryied to present, the most possible, how we get through this project, the most possible as a chronological way. Firstly, we analysed the data which were given to us, trying to understand it and notice the most relevant information from it for the model we were going to devellop. Secondly, we presented one of our biggest research lead that we tryied to devellop at the begining of the project. We had really good hope about it at the beginning. However, the methods we had to devellop to get through it were to much complicated so it was not a success. We thus, re-start from scratch with a more classical approach which consisted in using already made-up models, such as Random Forest, XGBoost, AutoML and trying to improve it. This approach were really efficient at the beginning. When we started to use it, we get really satisfying results. Then, we tried more and more some pre-processing ideas, which were presented in this parts above. Thus, some ideas which thought were improvement were not working well when we tryed it. Actually, some thing were missing in that approach, it was to fit the hyperparamters with this. We already tryed earlier to find the best hyperparameters but the first approach was not successful. We get a lot of improvement in the method developped in [XGBoost hyper-parameters](#62-xgboost-hyper-parameters). Though, we developped this method quite late compared to the deadline of the project, so we could not develop it as we willing to; the time requires to develop it takes a lot of time because of the exponential complexity of the algorithm. So, we could not tryed all the improvement we thought about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we are quite satisfiyed by our final results, concerning the short-time in which we developed the final model. However, we are quite disappointed because our first approach was not successful and we lost a lot of time on it. In further projects we would not realize the same approach. Making easy features at first is far more efficient that trying a big one in which we are not sure. This is something well known actually but it never been that true for us. We can of bet a lot on the first approach and it was not worth it, while the approach step by step was far more successful in the restricted time we had to develop it. Actually, our first experiment with XGBoost, our main program, started two weeks before the deadline, and our biggest improvement were in the last few days. So, probably with some more days, we could have improved it with what we said in the part [Areas for improvement](#7-areas-for-improvement). So, as a futher notice for us, we could have manage better our time by managing better our areas leads. That is how we conclude this project which were quite challenging but very exciting in terms of technical issue but also in terms of management issues. Even if our results are not the best, and could be seen as a failure from a specific point of view, we learnt a lot from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsolar",
   "language": "python",
   "name": "mlsolar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
