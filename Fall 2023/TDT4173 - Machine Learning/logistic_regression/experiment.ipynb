{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f3fffb-61c6-4654-ae32-3b7c40d90f78",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "For this problem, you will be implementing a Logistic Regression model for binary classification (yes-or-no like predictions). This is a simple model with restricted application areas, but it is very useful when applied to appropriate problems. Moreover, for those of you interested in Deep Learning, many of the concepts covered here are fundamental to how most neural networks as built and optimized.\n",
    "\n",
    "You should begin by implementing a standard form of logistic regression. That is, we want to predict a target variable $y \\in \\{0, 1\\}$ given observed feature variables $x \\in \\mathbb{R}^n$, and their relationship is modeled as:\n",
    "\n",
    "- prediction $ = \\hat{y} = h_\\theta(x) = \\sigma(\\theta^T x) \\in (0, 1)$, where:\n",
    "    - $h_\\theta$ is your Logistic Regression model\n",
    "    - $\\theta \\in \\mathbb{R}^n$ is the parameters of the model\n",
    "    - $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic function (`sigmoid` in the provided code)\n",
    "\n",
    "The parameters $\\theta$ should be optimized with gradient descent on a loss derived from maximum likelihood estimation under the assumption that $P(y | x, \\theta)$ is Bernoulli distributed.\n",
    "Please refer to [Andrew Ng's lecture notes on supervised learning](http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf) for more details. The most salient information for this problem can be found in Section 5, but we recommend that you read through all of it.\n",
    "\n",
    "We have provided some skeleton code for the classifier, along with a couple of utility functions in the [logistic_regression.py](./logistic_regression.py) module. Please fill out the functions marked with `TODO` and feel free to add extra constructor arguments as you see fit (just make sure the default constructor solves the first dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95fa0b-79f4-4a17-87da-30fb86e62a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb2e35-0307-4b2d-bc1c-f26a57b22e18",
   "metadata": {},
   "source": [
    "We begin by loading necessary packages. Below follows a short description of the imported modules:\n",
    "\n",
    "- `numpy` is the defacto python package for numerical calculation. Most other numerical libraries (including pandas) is based on numpy.\n",
    "- `pandas` is a widely used package for manipulating (mostly) tabular data\n",
    "- `matplotlib` is the most used plotting library for python\n",
    "- `seaborn` contains several convience functions for matplotlib and integrates very well with pandas\n",
    "- `logistic_regression` refers to the module in this folder that should be further implemented by you.\n",
    "\n",
    "Note: The `%autoreload` statement is an [IPython magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) that automatically reloads the newest version of all imported modules within the cell. This means that you can edit the `logistic_regression.py` file and just rerun this cell to get the updated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22739f54-37ef-45a1-8c40-8e28ea808f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logistic_regression as lr   # <-- Your implementation\n",
    "\n",
    "sns.set_style('darkgrid') # Seaborn plotting style "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2fd5b-02a5-413c-9547-9463056f2699",
   "metadata": {},
   "source": [
    "## [1] First Dataset\n",
    "\n",
    "The first dataset is a simple problem that can be used to debug your algorithm. The objective is to predict whether the dependent variable ($y$) should be equal to 1 or 0 based on two independent variables ($x_0$ and $x_1$).\n",
    "\n",
    "### [1.1] Load Data\n",
    "\n",
    "We begin by loading data from the .csv file located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd6e43-4a33-4101-9160-a732b3dc7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a52e8-3c35-4d4a-9903-a3b644f38dc4",
   "metadata": {},
   "source": [
    "### [1.2] Visualize\n",
    "\n",
    "Since the feature-space is continuous and 2-dimensional, it lends itself nicely to visualization with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d8d41-7cee-4f6d-8ac8-eb08b5306130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', data=data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39919c-df56-46a8-b6b2-ff4d5a23ee29",
   "metadata": {},
   "source": [
    "### [1.3] Train and Evaluate\n",
    "\n",
    "Next we fit and evaluate a Logistic Regression classifier over the dataset. We first partition the data into the dependent (`y`) and independent (`X`) variables. We then initialize a Logistic Regression classifier and fit it to all the data. Finally, we evaluate the model over the same data by calculating _accuracy_ and (binary) _cross entropy_.\n",
    "\n",
    "- The accuracy is given by the fraction of correctly classified samples. Since the Logistic Regression classifier outputs \"soft\" predictions ($\\hat{y} \\in (0, 1)$), we threshold the predictions so that $\\hat{y} \\geq 0.5$ is considered a $1$ and $\\hat{y} < 0.5$ is considered a $0$.\n",
    "\n",
    "- The [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) measures the dissimilarity between the distributions of the true labels ($y$) and the \"label-probablities\" ($\\hat{y}$) predicted by your model. It is subject to minimization and (hint) _highly_ related to your maximal likelihood objective.\n",
    "\n",
    "Note that `.fit` and `.predict` will crash until you implement these two methods in [logistic_regression.py](./logistic_regression.py).\n",
    "\n",
    "Assuming a standard implementation of logistic regression with batch gradient descent, you should expect to get an accuracy of at least 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70f628",
   "metadata": {},
   "source": [
    "With weights initialized manually we get those results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35312b1-e61d-48e0-b707-21a98b8acfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into independent (feature) and depended (target) variables\n",
    "X = data_1[['x0', 'x1']]\n",
    "y = data_1['y']\n",
    "\n",
    "# Make the data usable\n",
    "X, y = np.array(X), np.array(y)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "# Create and train model.\n",
    "model_1 = lr.LogisticRegression() \n",
    "model_1.fit(X, y, init_mode='manual')\n",
    "\n",
    "# Calculate accuracy and cross entropy for (insample) predictions \n",
    "y_pred = model_1.predict(X)\n",
    "print(f'Accuracy: {lr.binary_accuracy(y_true=y, y_pred=y_pred, threshold=0.5) :.3f}')\n",
    "print(f'Cross Entropy: {lr.binary_cross_entropy(y_true=y, y_pred=y_pred) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ec3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.scatter(X[:,0], X[:, 1], c=y, cmap='summer')\n",
    "\n",
    "b = model_1.b\n",
    "W = model_1.weights\n",
    "print(W.shape)\n",
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = - ( W[0] * x1 + b) / W[1]\n",
    "\n",
    "ax.plot(x1, x2, c='orange', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595045c1",
   "metadata": {},
   "source": [
    "With weights initialized randomly we get those results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e826ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into independent (feature) and depended (target) variables\n",
    "X = data_1[['x0', 'x1']]\n",
    "y = data_1['y']\n",
    "\n",
    "# Make the data usable\n",
    "X, y = np.array(X), np.array(y)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "# Create and train model.\n",
    "model_1 = lr.LogisticRegression() \n",
    "model_1.fit(X, y, lr=1, nb_epochs=100, init_mode='auto')\n",
    "\n",
    "# Calculate accuracy and cross entropy for (insample) predictions \n",
    "y_pred = model_1.predict(X)\n",
    "# print(y_pred, y)\n",
    "print(f'Accuracy: {lr.binary_accuracy(y_true=y, y_pred=y_pred, threshold=0.5) :.3f}')\n",
    "print(f'Cross Entropy: {lr.binary_cross_entropy(y_true=y, y_pred=y_pred) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a26f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.scatter(X[:,0], X[:, 1], c=y, cmap='summer')\n",
    "\n",
    "b = model_1.b\n",
    "W = model_1.weights\n",
    "print(W.shape)\n",
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = - ( W[0] * x1 + b) / W[1]\n",
    "\n",
    "ax.plot(x1, x2, c='orange', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e47ff-a284-4fd0-a8f2-d560bc1df691",
   "metadata": {},
   "source": [
    "### [1.4] Visualize Decision Boundary\n",
    "\n",
    "Logistic Regression does not make hard classification decisions. Instead, it attempts to model the probability of a datapoint belonging to the 1-class after conditioning on the available features ($x_0, x_1$ here). The modeled probability of the 0-class can be obtained by negating and adding one.\n",
    "\n",
    "We can visualize the modeled probabilities across the entire input space. In the cell below, we rasterize the model's prediction over a grid that covers approximately the same area as the data used to train it. Red regions correspond to high modeled probability for the 1-class whereas blue correspond to high modeled probability for the 0-class. Does the decisions of your model line up well with the provided data? What happens if you train it for longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc7530-b1e0-4451-89ed-c70954595b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the model's predictions over a grid\n",
    "xx0, xx1 = np.meshgrid(np.linspace(-0.1, 1.1, 100), np.linspace(-0.1, 1.1, 100))\n",
    "yy = model_1.predict(np.stack([xx0, xx1], axis=-1).reshape(-1, 2)).reshape(xx0.shape)\n",
    "\n",
    "# Plot prediction countours along with datapoints\n",
    "_, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "levels = [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1.0]\n",
    "contours = ax.contourf(xx0, xx1, yy, levels=levels, alpha=0.4, cmap='RdBu_r', vmin=0, vmax=1)\n",
    "legends = [plt.Rectangle((0,0),1,1,fc = pc.get_facecolor()[0]) for pc in contours.collections]\n",
    "labels = [f'{a :.2f} - {b :.2f}' for a,b in zip(levels, levels[1:])]\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', ax=ax, data=data_1)\n",
    "ax.legend(legends, labels, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647d426-44a8-4364-adef-e801028caf1e",
   "metadata": {},
   "source": [
    "## [2] Second Dataset\n",
    "\n",
    "The second dataset is superficially similar to the first one. The objective is still to predict whether the dependent variable ($y$) should be equal to 1 or 0 based on two independent variables ($x_0$ and $x_1$). However, it is designed to be a bit more challenging. You might want to explore it beyond what is provided in the cells below.\n",
    "\n",
    "### [2.1] Load Data\n",
    "\n",
    "This dataset can also be found in a .csv file in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d352b-6266-4542-89d5-b42be8c5e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second dataset and partition into train/test split\n",
    "data_2 = pd.read_csv('data_2.csv')\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6dfd8-61d0-4a02-b382-2d94b4c7c680",
   "metadata": {},
   "source": [
    "### [2.2] Split Data\n",
    "\n",
    "The dataset is partitioned into two groups:\n",
    "\n",
    "- `train` contains 500 samples that you should use to fit the model\n",
    "- `test` contains another 500 samples that should only be used to check that your solution generalizes well to novel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af5570-0e0e-4376-ac3d-33c475df7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_train = data_2.query('split == \"train\"')\n",
    "data_2_test = data_2.query('split == \"test\"')\n",
    "\n",
    "# Partition data into independent (features) and depended (targets) variables\n",
    "X_train, y_train = data_2_train[['x0', 'x1']], data_2_train['y']\n",
    "X_test, y_test = data_2_test[['x0', 'x1']], data_2_test['y']\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "y_train, y_test = y_train.reshape(y_train.shape[0], 1), y_test.reshape(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', data=data_2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', data=data_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d0b1f-a93d-4d30-aa45-d867438f6b29",
   "metadata": {},
   "source": [
    "### [2.3] Fit and Evaluate Model\n",
    "\n",
    "You may notice that the algorithm that worked pretty well on the first dataset comes up a bit short here. Feel free to add extra functionality to it and/or the data preprocessing pipeline that might improve performance. As a debugging reference; it is possible to obtain accuracies over 80%, both on train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab167cae-ceb2-4162-a1b7-7f41e9fab402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model (TO TRAIN SET ONLY)\n",
    "learning_rate = 0.8\n",
    "epochs = 100000\n",
    "model_2 = lr.LogisticRegression(dim=12)\n",
    "model_2.fit(X_train, y_train, lr=learning_rate, nb_epochs=epochs)\n",
    "\n",
    "# Calculate accuracy and cross entropy for insample predictions \n",
    "y_pred_train = model_2.predict(X_train)\n",
    "print('Train')\n",
    "print(f'Accuracy: {lr.binary_accuracy(y_true=y_train, y_pred=y_pred_train, threshold=0.5) :.3f}')\n",
    "print(f'Cross Entropy:  {lr.binary_cross_entropy(y_true=y_train, y_pred=y_pred_train) :.3f}')\n",
    "\n",
    "# Calculate accuracy and cross entropy for out-of-sample predictions\n",
    "y_pred_test = model_2.predict(X_test)\n",
    "print('\\nTest')\n",
    "print(f'Accuracy: {lr.binary_accuracy(y_true=y_test, y_pred=y_pred_test, threshold=0.5) :.3f}')\n",
    "print(f'Cross Entropy:  {lr.binary_cross_entropy(y_true=y_test, y_pred=y_pred_test) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model_2.b\n",
    "W = model_2.weights\n",
    "\n",
    "x0 = np.linspace(0, 1, 100)\n",
    "\n",
    "a0 = W[3]\n",
    "b0 = W[2]\n",
    "c0 = W[0] * x0 + W[1] * ( x0 ** 2 ) + b\n",
    "delta = b0 ** 2 - 4 * a0 * c0\n",
    "\n",
    "x121 = ( - b0 - np.sqrt(delta) ) / ( 2 * W[3] )\n",
    "x122 = ( - b0 + np.sqrt(delta) ) / ( 2 * W[3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49af2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_23 = lr.LogisticRegression(dim=3)\n",
    "model_23.fit(X_train, y_train, lr=learning_rate, nb_epochs=epochs, plot=False)\n",
    "\n",
    "b = model_23.b\n",
    "W = model_23.weights\n",
    "\n",
    "x0 = np.linspace(0, 1, 100)\n",
    "\n",
    "# We use the following package only to solve \n",
    "# wT.X = 0 in order to plot the model, it would\n",
    "# not be useful otherwise\n",
    "\n",
    "import sympy as sy\n",
    "\n",
    "x1 = sy.Symbol('x1')\n",
    "\n",
    "eqs = np.array([sy.Eq( x1 ** 3 + W[][0] / W[5][0] * x1 ** 2 + W[3][0] / W[5][0] * x1 + W[0][0] / W[5][0] * x + W[1][0] / W[5][0] * x ** 2 + W[2][0] / W[5][0] * x ** 3 + b / W[5][0], 0) for x in x0])\n",
    "\n",
    "x1 = np.array([sy.solve(eq) for eq in eqs]).T\n",
    "x131, x132, x133 = x1\n",
    "\n",
    "x131 = np.array([sy.re(x) for x in x131])\n",
    "x132 = np.array([sy.re(x) for x in x132])\n",
    "x133 = np.array([sy.re(x) for x in x133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75850aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots to compare model on orders 2 and 3\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[:,0], X_train[:, 1], c=y_train, cmap='summer')\n",
    "plt.plot(x0, x121, c='orange', lw=3)\n",
    "plt.plot(x0, x122, c='orange', lw=3)\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train[:,0], X_train[:, 1], c=y_train, cmap='summer')\n",
    "plt.plot(x0, x132, c='orange', lw=3)\n",
    "plt.plot(x0, x133, c='orange', lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_train, accs_test = [], []\n",
    "losss_train, losss_test = [], []\n",
    "h_losss_train, h_losss_test = [], []\n",
    "\n",
    "x_abs = []\n",
    "for k in range(1 , 16):\n",
    "    # print('dim: ', k)\n",
    "    x_abs.append(k)\n",
    "    model_2 = lr.LogisticRegression(dim=k)\n",
    "    model_2.fit(X_train, y_train, lr=learning_rate, nb_epochs=epochs, plot=False)\n",
    "\n",
    "    y_pred_train = model_2.predict(X_train)\n",
    "    y_pred_test = model_2.predict(X_test)\n",
    "\n",
    "    accs_train.append(lr.binary_accuracy(y_true=y_train, y_pred=y_pred_train, threshold=0.5))\n",
    "    accs_test.append(lr.binary_accuracy(y_true=y_test, y_pred=y_pred_test, threshold=0.5))\n",
    "    losss_train.append(lr.binary_accuracy(y_true=y_train, y_pred=y_pred_train))\n",
    "    losss_test.append(lr.binary_cross_entropy(y_true=y_test, y_pred=y_pred_test))\n",
    "    h_losss_train.append(lr.hinge_loss(y_true=y_train, y_pred=y_pred_train))\n",
    "    h_losss_test.append(lr.hinge_loss(y_true=y_test, y_pred=y_pred_test))\n",
    "\n",
    "nb_column = 2\n",
    "\n",
    "h_losss_train = np.array(h_losss_train).reshape(-1)\n",
    "h_losss_test = np.array(h_losss_test).reshape(-1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_abs, accs_train, label='trainset accuracy')\n",
    "plt.plot(x_abs, accs_test, label='testset accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_abs, losss_train, label='trainset loss')\n",
    "plt.plot(x_abs, losss_test, label='testset loss')\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.plot(x_abs, h_losss_train, label='trainset hinge loss')\n",
    "# plt.plot(x_abs, h_losss_test, label='testset hinge loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f675d-f3ca-4a69-8bdf-7e13254f7e23",
   "metadata": {},
   "source": [
    "## [3] Further Steps (optional)\n",
    "\n",
    "If you're done with the assignment but want to some more challenges; consider the following:\n",
    "\n",
    "- Try a different objective function (e.g. [hinge-loss](https://en.wikipedia.org/wiki/Hinge_loss)). How does this affect the classifier?\n",
    "- Try a more sophisticated stopping criterion than a fixed number of epochs\n",
    "- Try a different optimization algorithm than simple gradient descent, for instance:\n",
    "    - The [ADAM](https://optimization.cbe.cornell.edu/index.php?title=Adam) optimizer is a variation of gradient descent that is quite popular in deep learning.\n",
    "    - The quasi-newton method [BGFS](https://optimization.cbe.cornell.edu/index.php?title=Quasi-Newton_methods#BFGS_method), which typically converge much faster\n",
    "- Try to generalize your classifier to a [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) neural network classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3564b",
   "metadata": {},
   "source": [
    "### [3.1] Different objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.hinge_loss(y_true=y_train, y_pred=y_pred_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
