{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_autoencoder import VariationalAutoEncoder\n",
    "from verification_net import VerificationNet\n",
    "from model_trainer import ModelTrainer\n",
    "from stacked_mnist import StackedMNIST, DataMode\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "torch.mps.set_per_process_memory_fraction(0.)\n",
    "\n",
    "trainer_file = Path(\"trainers/vae-basic.pkl\")\n",
    "model_file = Path(\"models/vae-basic\")\n",
    "\n",
    "latent_space_size = 64\n",
    "\n",
    "mode = DataMode.MONO | DataMode.BINARY\n",
    "\n",
    "trainset = StackedMNIST(train=True, mode=mode)\n",
    "testset = StackedMNIST(train=False, mode=mode)\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset, shuffle=True, batch_size=2048)\n",
    "test_loader = DataLoader(dataset=testset, shuffle=True, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderTrainer(ModelTrainer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            loss, \n",
    "            optimizer,\n",
    "            device = torch.device(\"mps\"),\n",
    "            file_name: str | Path = model_file, \n",
    "            force_learn: bool = False\n",
    "        ) -> None:\n",
    "        super().__init__(model, loss, optimizer, device, file_name, force_learn)\n",
    "\n",
    "    def get_output_from_batch(self, batch):\n",
    "        x, _, _ = batch\n",
    "        x = x.to(self.device)\n",
    "        (mu, log_var), x_hat = self.model(x)\n",
    "        return (x_hat, x), (mu, log_var)\n",
    "    \n",
    "def loss(X, params):\n",
    "    x_hat, x = X\n",
    "    mu, log_var = params\n",
    "    BCE = F.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "    KLD = torch.mean(- 0.5 * torch.mean(1 + log_var - mu.pow(2) - torch.exp(log_var), axis=1))\n",
    "\n",
    "    return BCE + .02 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = VariationalAutoEncoder(latent_space_size=latent_space_size)\n",
    "opt = optim.Adam(VAE.parameters(), lr=1e-6)\n",
    "Trainer = VariationalAutoEncoderTrainer(\n",
    "        model=VAE, \n",
    "        loss=loss, \n",
    "        optimizer=opt, \n",
    "        file_name=model_file,\n",
    "        force_learn=False\n",
    "    )\n",
    "Trainer = Trainer.load_trainer(trainer_file=trainer_file)\n",
    "\n",
    "batch_size = 1\n",
    "data = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "x, _, labels = next(iter(data))\n",
    "\n",
    "plt.plot(Trainer.losses, label=\"train loss\")\n",
    "plt.plot(Trainer.val_losses, label=\"val loss\")\n",
    "plt.legend()\n",
    "\n",
    "Trainer.print_reconstructed_img(trainset, batch_size=16)\n",
    "\n",
    "VerifNet = VerificationNet(force_learn=False, file_name='models/verification_model_torch_ok_copy')\n",
    "Trainer.print_class_coverage_and_predictability(VerifNet, dataset=trainset, batch_size=10_000)\n",
    "Trainer.print_class_coverage_and_predictability(VerifNet, testset, batch_size=10_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
