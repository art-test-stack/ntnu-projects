{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_trainer import ModelTrainer\n",
    "from verification_net import VerificationNet\n",
    "from stacked_mnist import StackedMNIST, DataMode\n",
    "\n",
    "from autoencoder import AutoEncoder\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "torch.mps.set_per_process_memory_fraction(0.)\n",
    "\n",
    "latent_space_size = 64\n",
    "mode = DataMode.MONO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer(ModelTrainer):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            loss, \n",
    "            optimizer,\n",
    "            device = torch.device(\"mps\"),\n",
    "            file_name: str | Path = 'models/ae-basic', \n",
    "            force_learn: bool = False\n",
    "        ) -> None:\n",
    "        super().__init__(model, loss, optimizer, device, file_name, force_learn)\n",
    "\n",
    "    def get_output_from_batch(self, batch):\n",
    "        x, _, _ = batch\n",
    "        x = x.to(self.device)\n",
    "        _, output = self.model(x)\n",
    "        return x, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = AutoEncoder(latent_space_size=latent_space_size)\n",
    "AE.to(device)\n",
    "loss = nn.MSELoss()\n",
    "opt = optim.Adam(AE.parameters(), lr=1e-5)\n",
    "\n",
    "Trainer = AutoEncoderTrainer(\n",
    "        model=AE, \n",
    "        loss=loss, \n",
    "        optimizer=opt, \n",
    "        file_name='models/ae-basic',\n",
    "        force_learn=False\n",
    "    )\n",
    "\n",
    "batch_size = 20_000\n",
    "noise_img = nn.Softmax(dim=1)(torch.rand(batch_size, latent_space_size))\n",
    "\n",
    "noise_pred = AE.decoder(noise_img.to(device))\n",
    "\n",
    "VerifNet = VerificationNet(file_name='models/verification_model_torch_ok_copy')\n",
    "\n",
    "noise_labels, beliefs = VerifNet.predict(noise_pred)\n",
    "\n",
    "cov = VerifNet.check_class_coverage(data=noise_pred, tolerance=.8)\n",
    "pred, acc = VerifNet.check_predictability(data=noise_pred, tolerance=.8)\n",
    "print(f\"Coverage: {100*cov:.2f}%\")\n",
    "print(f\"Predictability: {100*pred:.2f}%\")\n",
    "\n",
    "from util import tile_tv_images\n",
    "arg_beliefs = np.flip(np.argsort(beliefs))[:16]\n",
    "imgs_gen_to_plot = noise_pred.reshape(-1, 28, 28).to(\"cpu\").detach().numpy()[arg_beliefs]\n",
    "\n",
    "tile_tv_images(images=imgs_gen_to_plot, labels=noise_labels[arg_beliefs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
